{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU5fRQwhEdJy"
   },
   "source": [
    "# Subword Tokenization\n",
    "\n",
    "In this exercise, we will learn how to train our own subword tokenizers with different algorithms: BPE and Unigram. We will use `sentencepiece`, a library from Google to help create our tokenizers.\n",
    "\n",
    "## Ref:\n",
    "https://github.com/google/sentencepiece/blob/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI9gRZlUE80g"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1pOsV-jaW975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-17 22:39:55--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt [following]\n",
      "--2025-01-17 22:39:56--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3231076 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘pra-apai-manee-ch1-50.txt.2’\n",
      "\n",
      "pra-apai-manee-ch1- 100%[===================>]   3.08M  17.3MB/s    in 0.2s    \n",
      "\n",
      "2025-01-17 22:39:57 (17.3 MB/s) - ‘pra-apai-manee-ch1-50.txt.2’ saved [3231076/3231076]\n",
      "\n",
      "--2025-01-17 22:39:57--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/kratoo-40000000-40002000.jsonl [following]\n",
      "--2025-01-17 22:39:58--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/kratoo-40000000-40002000.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2968483 (2.8M) [text/plain]\n",
      "Saving to: ‘kratoo-40000000-40002000.jsonl.2’\n",
      "\n",
      "kratoo-40000000-400 100%[===================>]   2.83M  17.9MB/s    in 0.2s    \n",
      "\n",
      "2025-01-17 22:39:59 (17.9 MB/s) - ‘kratoo-40000000-40002000.jsonl.2’ saved [2968483/2968483]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
    "!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSiDpG9WE-cT"
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OQd7M6gLWPLN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q sentencepiece\n",
    "import sentencepiece as spm\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OifbmMIstzs8"
   },
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-FnIDvb1lMuh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060318"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pantip_text = []\n",
    "with open(\"kratoo-40000000-40002000.jsonl\", \"r\") as json_file:\n",
    "    json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        pantip_text.append(f\"{result['title']}\\n{result['content']}\\n\")\n",
    "sum([len(t) for t in pantip_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yaaQVXZ8A0j1"
   },
   "outputs": [],
   "source": [
    "with open(\"pra-apai-manee-ch1-50.txt\") as f:\n",
    "    pra_apai_manee_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LksJKc9MA5F_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100605"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(t) for t in pra_apai_manee_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RbdfkF-vAoie"
   },
   "outputs": [],
   "source": [
    "pantip_train_text = pantip_text[: int(len(pantip_text) * 0.8)]\n",
    "pantip_test_text = pantip_text[int(len(pantip_text) * 0.8) :]\n",
    "\n",
    "pam_train_text = pra_apai_manee_data[: int(len(pra_apai_manee_data) * 0.8)]  # pam = pra_apai_manee\n",
    "pam_test_text = pra_apai_manee_data[int(len(pra_apai_manee_data) * 0.8) :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhwcH0Aot1XI"
   },
   "source": [
    "## Run tokenizer training\n",
    "\n",
    "The Python wrapper provides multiple APIs for training our tokenizers\n",
    "\n",
    "1. `spm.SentencePieceTrainer.train(input='input.txt', model_prefix='m', vocab_size=vocab_size, model_type=model_type)`\n",
    "  <br> This will output the tokenizer files `m.model` and `m.vocab` that can be later loaded into `SentencePieceProcessor`.\n",
    "  <br><br>\n",
    "2. `spm.SentencePieceTrainer.train(sentence_iterator=iterator, model_writer=obj_with_write_method, vocab_size=vocab_size, model_type=model_type)`\n",
    "  <br> This method will require a file object e.g. `obj_with_write_method = io.BytesIO()`. The advantage of this method is you can run sentencepiece on environments that have limited access to the local file system. But you will still have to save the model file if you want to re-use the model else you will have to train it again.\n",
    "<br><br>\n",
    "3.  `spm.SentencePieceTrainer.train('--input=input.txt --model_prefix=m --vocab_size=vocab_size --model_type=model_type')`\n",
    "<br> Same as no.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3XeFFYw-T_0"
   },
   "source": [
    "### Unigram tokenizer\n",
    "\n",
    "We are going to start with training a unigram tokenizer. You can use any method of training one. Make sure to set vocab_size to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bFCfHphd15g9"
   },
   "outputs": [],
   "source": [
    "both_train = pantip_train_text + pam_train_text\n",
    "both_test = pantip_test_text + pam_test_text\n",
    "\n",
    "both_train_corpus = \"\\n\".join(both_train)\n",
    "both_test_corpus = \"\\n\".join(both_test)\n",
    "\n",
    "with open(\"both_train_corpus.txt\", \"w\") as f:\n",
    "    f.write(both_train_corpus)\n",
    "with open(\"both_test_corpus.txt\", \"w\") as f:\n",
    "    f.write(both_test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam_train_corpus = \"\\n\".join(pam_train_text)\n",
    "pam_test_corpus = \"\\n\".join(pam_test_text)\n",
    "\n",
    "with open(\"pam_train_corpus.txt\", \"w\") as f:\n",
    "    f.write(pam_train_corpus)\n",
    "with open(\"pam_test_corpus.txt\", \"w\") as f:\n",
    "    f.write(pam_test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: pam_train_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: pam_unigram\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: pam_train_corpus.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 16314 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=885139\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.954% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99954\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 16314 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=411778\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 275348 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 16314\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 32181\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 32181 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=49394 obj=42.2355 num_tokens=128673 num_tokens/piece=2.60503\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=40979 obj=36.7371 num_tokens=130046 num_tokens/piece=3.17348\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=30367 obj=36.9771 num_tokens=139359 num_tokens/piece=4.58916\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=29880 obj=36.5016 num_tokens=139582 num_tokens/piece=4.67142\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22205 obj=37.6771 num_tokens=149629 num_tokens/piece=6.73853\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22104 obj=37.2684 num_tokens=149717 num_tokens/piece=6.7733\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=16512 obj=38.7389 num_tokens=161069 num_tokens/piece=9.75466\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16473 obj=38.3341 num_tokens=161183 num_tokens/piece=9.78468\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12338 obj=40.0409 num_tokens=172981 num_tokens/piece=14.0202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12331 obj=39.6638 num_tokens=172997 num_tokens/piece=14.0294\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9245 obj=41.4482 num_tokens=185092 num_tokens/piece=20.0208\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9242 obj=41.1146 num_tokens=185096 num_tokens/piece=20.0277\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6930 obj=43.0791 num_tokens=198272 num_tokens/piece=28.6107\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6930 obj=42.7435 num_tokens=198274 num_tokens/piece=28.611\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5196 obj=44.8072 num_tokens=211985 num_tokens/piece=40.7977\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5196 obj=44.4723 num_tokens=211972 num_tokens/piece=40.7952\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3897 obj=46.7216 num_tokens=226813 num_tokens/piece=58.202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3897 obj=46.3429 num_tokens=226820 num_tokens/piece=58.2037\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2922 obj=48.8639 num_tokens=244481 num_tokens/piece=83.6691\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2922 obj=48.4277 num_tokens=244676 num_tokens/piece=83.7358\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2191 obj=51.2638 num_tokens=265878 num_tokens/piece=121.35\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2191 obj=50.7382 num_tokens=265883 num_tokens/piece=121.352\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1643 obj=53.8622 num_tokens=291545 num_tokens/piece=177.447\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1643 obj=53.2395 num_tokens=291545 num_tokens/piece=177.447\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1232 obj=56.564 num_tokens=320433 num_tokens/piece=260.092\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1232 obj=55.8787 num_tokens=320433 num_tokens/piece=260.092\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=57.1231 num_tokens=332377 num_tokens/piece=302.161\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=56.8968 num_tokens=332377 num_tokens/piece=302.161\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: pam_unigram.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: pam_unigram.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"pam_train_corpus.txt\", model_prefix=\"pam_unigram\", vocab_size=1000, model_type=\"unigram\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdXPaoW3_v2T"
   },
   "source": [
    "### Q1 MCV\n",
    "\n",
    "How many tokens did you get when tokenizing the following sentence with your unigram tokenizer: <br>\n",
    "'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam_unigram_tokenizer = spm.SentencePieceProcessor(model_file=\"pam_unigram.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "J1bO3s-z-PLb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pam_unigram_tokenizer.encode(\"อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม\", out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKkc1D-hAFxl"
   },
   "source": [
    "### BPE Tokenizer\n",
    "\n",
    "Now try training a BPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AiXj57rh-PIv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: pam_train_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: pam_bpe\n",
      "  model_type: BPE\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: pam_train_corpus.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 16314 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=885139\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.954% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99954\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 16314 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 16314\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 32181\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8839 min_freq=120\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3834 size=20 all=3652 active=2394 piece=ให\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2412 size=40 all=5419 active=4161 piece=้อง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1781 size=60 all=7328 active=6070 piece=ได\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1469 size=80 all=9357 active=8099 piece=▁พระ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1191 size=100 all=11534 active=10276 piece=าว\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1168 min_freq=139\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1002 size=120 all=13824 active=3191 piece=ูก\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=876 size=140 all=16549 active=5916 piece=เม\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=748 size=160 all=19242 active=8609 piece=ความ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=677 size=180 all=21716 active=11083 piece=กํา\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=626 size=200 all=24479 active=13846 piece=สาร\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=623 min_freq=85\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=566 size=220 all=26945 active=3535 piece=ขึ้น\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=509 size=240 all=29932 active=6522 piece=รบ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=470 size=260 all=32718 active=9308 piece=แล\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=436 size=280 all=35246 active=11836 piece=้ํา\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=404 size=300 all=37519 active=14109 piece=ฝ้า\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=404 min_freq=48\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=382 size=320 all=39880 active=4139 piece=น้อย\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=356 size=340 all=42278 active=6537 piece=นึก\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=336 size=360 all=44997 active=9256 piece=แท\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=320 size=380 all=47070 active=11329 piece=ขัด\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=301 size=400 all=49390 active=13649 piece=ตรา\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=300 min_freq=33\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=285 size=420 all=51579 active=4541 piece=พู\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=269 size=440 all=53523 active=6485 piece=เลี้ยง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=257 size=460 all=55855 active=8817 piece=่าว\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=244 size=480 all=57964 active=10926 piece=▁ก็\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=234 size=500 all=60309 active=13271 piece=ียก\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=234 min_freq=25\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=224 size=520 all=62342 active=4958 piece=าะ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=205 size=540 all=64337 active=6953 piece=รุ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=199 size=560 all=66395 active=9011 piece=วาท\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=194 size=580 all=68294 active=10910 piece=อ่อน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=186 size=600 all=70205 active=12821 piece=ชน\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=186 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=178 size=620 all=71920 active=5149 piece=▁ซึ่ง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=171 size=640 all=73659 active=6888 piece=พักตร์\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=164 size=660 all=75411 active=8640 piece=▁ขึ้น\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=159 size=680 all=77310 active=10539 piece=พรั่ง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=155 size=700 all=79085 active=12314 piece=อื้น\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=155 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=149 size=720 all=80846 active=5691 piece=สัก\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=144 size=740 all=82546 active=7391 piece=เชษฐา\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=140 size=760 all=84297 active=9142 piece=▁กระ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=136 size=780 all=85826 active=10671 piece=กําปั่น\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=132 size=800 all=87264 active=12109 piece=▁ศรีสุวรรณ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=131 min_freq=15\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=128 size=820 all=88742 active=5808 piece=สบาย\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=123 size=840 all=90098 active=7164 piece=ไหล\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=119 size=860 all=91598 active=8664 piece=ชาติ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=880 all=93143 active=10209 piece=ไท\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=113 size=900 all=94526 active=11592 piece=คํานับ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=113 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=920 all=95958 active=6087 piece=แป\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: pam_bpe.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: pam_bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"pam_train_corpus.txt\",\n",
    "    model_prefix=\"pam_bpe\",\n",
    "    vocab_size=1000,\n",
    "    model_type=\"bpe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrQwGmL5AMXc"
   },
   "source": [
    "### Q2 MCV\n",
    "\n",
    "How many tokens did you get when tokenizing the following sentence with your BPE tokenizer: <br>\n",
    "'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pam_bpe_tokenizer = spm.SentencePieceProcessor(model_file=\"pam_bpe.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0AXuzyaN-PEr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pam_bpe_tokenizer.encode(\"อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม\", out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbb6C6-IS_Ly"
   },
   "source": [
    "These are some of your vocabs. Note that you will see \"▁\" (U+2581) in every type of tokenizer in SentencePiece since it makes it possible to perform detokenization \\(unsplit your sentences\\) without relying on language-specific resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Aa9j6XrTKjyA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> | <s> | </s> | ▁ | า | เ | น | ม | ย | ก | ร | ว | ด | ส | ง | บ | ค | มา | อ | ล | จะ | ท | ให้ | ห | ไป | ไม่ | แ | ว่า | พ | ุ | ี | ๏ | ฯ | ข | ช | เป็น | พระ | โ | ที่ | ใจ | ▁จะ | จ | ะ | ิ | ต | ก็ | อยู่ | ป | ได้ | ่ | ไ | เข้า | ู | ▁พระ | ้า | ตาม | ใน | ้ | ▁แล้ว | เหมือน | รา | ศ | เจ้า | เห็น | ลา | กัน | ั | หา | นาง | ทรง | ประ | ์ | ยา | ัก | ํา | ซ | าน | ัง | ฉ | องค์ | ัด | แล้ว | อน | ดู | ถ | ด้วย | มี | ▁จึง | นี้ | ่า | ผ | น้อง | แต่ | ทํา | ▁นาง | ▁ให้ | รัก | พี่ | คิด | ลูก | พา | รู้ | การ | กับ | ัน | หน้า | กระ | วน | ออก | ่อ | เขา | ถึง | ระ | ข้า | ับ | พล | นั่ง | ทั้ง | หน | รับ | ษ | กล | วง | ลง | ฝ | กร | พร | ความ | เสีย | ดี | ขึ้น | อง | ่ง | ธ | ▁แต่ | คน | กลับ | ▁ฝ่าย | ้น | อด | ภ | หรือ | ตร | ือ | ฟัง | แม่ | ▁ไม่ | ไว้ | ยัง | ▁เห็น | นา | ขอ | มิ | น้ํา | หล | ดัง | ▁พอ | ▁ทั้ง | ช่วย | สม | นั้น | ริ | ทัพ | ต้อง | วัน | อา | น้อย | รบ | ิน | อย่า | เอา | จน | เรา | สุด | เสียง | ข้าง | หลัง | ตี | ตัว | ละ | สุ | วัง | ทุก | ่น | ึก | นึก | เฝ้า | นาย | ฝรั่ง | ทูล | เส | วิ | ปล | ▁ถึง | ตาย | ใคร | อก | อั | ตา | เรือ | จึง | แล | ี่ | ั่ง | แสน | สอง | ของ | ็ | ลี | ี้ | จิต | หมาย | ้ม | แจ้ง | ั่น | สั่ง | ราช | พิ | เห | หาย | ้อง | เมือง | เหลือ | กลาง | กษัตริย์ | ยิ่ง | ตรัส | ึง | เลย | เล่า | ทาง | ุด | ศรี | เคย | ไหน | สาม | หนี | ณ | มัน | ื้อ | ค่อย | ชาย | พราหมณ์ | ▁อย่า | ญ | ที | นิ | น่า | สิ้น | ฉัน | กาย | ลังกา | ▁ด้วย | คอย | บอก | สิ | ฟ | สงสาร | พ่อ | ยง | จริง | ชาว | ถาม | ไร | ทหาร | ตั้ง | ▁อัน | เที่ยว | ปร | ผู้ | พวก | สาร | ชม | ศึก | คํา | ▁เป็น | ทอง | อบ | ใหญ่ | ถือ | สาว | พระอภัย | จง | สา | จับ | ั้น | พลาง | ▁มา | ยก | ▁บ้าง | ไพร่ | ลม | ล้วน | ▁ต่าง | ร้อย | พบ | งาม | แกล้ง | อาย | จะได้ | เคียง | อย่าง | เครื่อง | กลัว | ลาย | จํา | ต่าง | สินสมุทร | ▁พวก | ม้า | ลํา | นี่ | ผา | แก้ว | เพราะ | ▁ครั้น | ▁จน | ▁แม้น | สาย | พัน | พระองค์ | พร้อม | วาย | ชิง | ห้อง | ร้อง | สู้ | ▁จง | ลิ | ราย | ล่อ | จาก | ้ว | ท่าน | รอง | เดิน | เรียก | ขัด | เหล่า | กุมาร | ผล | ป่า | ู่ | คู่ | รูป | กิน | พอ | ร่ํา | โฉม | ▁ถ้า | คง | ่าย | ใช้ | ตอบ | หลง | ไล่ | จัด | ดับ | ▁เมื่อ | บน | อ่อน | แสง | คืน | ใส่ | แค้น | รถ | ตรง | แต่ง | แน่ | เชิญ | ชื่น | ถวาย | โห | จร | มิได้ | นอน | ุก | ชวน | เมีย | อาลัย | ้อม | ลับ | ไหว | ▁แม้ | บิดา | หญิง | หลับ | ดอก | กล้า | ขาด | จัก | ไม่มี | บาท | เสนา | ย์ | ช่าง | โศก | วาง | ติด | เสร็จ | ร้อน | คุณ | ผัว | นัก | ความตาม | พักตร์ | หน่อ | ้อย | ▁ซึ่ง | ตะ | ห้าม | พราย | ฟ้า | ไฉน | ใ | ตก | เมื่อ | ยศ | ชล | ดํา | หนึ่ง | ผัน | ใด | สัก | ร้าย | วิ่ง | แก้ | ยาม | ศรีสุวรรณ | ปืน | ฆ่า | ขับ | ขวา | ไฟ | พูด | หมอง | ก็ไม่ | กําลัง | รักษา | เช่น | ุ่ม | ผี | หาญ | เล่น | เนื้อ | รีบ | ถูก | ชัย | บุตรี | ฟัน | บ้าง | เอ๋ย | สงสัย | ผิด | นิ่ง | ชื่อ | เถิด | ผ่อน | หลาน | สี่ | ชาติ | อี | ปาก | ช้า | ึ | แตก | ตรา | รณ | ลอง | ปี | หมอ | เจ้าพราหมณ์ | พี่เลี้ยง | ต่อ | พลอย | โฉมยง | เนตร | หัก | กอด | เชย | ทั้งสอง | ยิ้ม | ค่ํา | นอก | ขวัญ | ซ้ํา | อารมณ์ | ทุกข์ | แขก | เย็น | หนักหนา | ั้ง | ปิด | โปรด | ้ง | กําปั่น | เรียง | แรง | สิ่ง | เศร้า'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_vocabs = [pam_unigram_tokenizer.id_to_piece(id) for id in range(pam_unigram_tokenizer.get_piece_size())]\n",
    "\" | \".join(unigram_vocabs[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2TsXA0UqN5LN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> | <s> | </s> | ้า | ่า | อง | ระ | ํา | รา | อย | ่ง | มา | จะ | ัง | ัน | ▁เ | าย | ้ว | ับ | ี่ | ม่ | อน | ให | าม | ้น | ็น | พระ | ีย | าง | กล | ้ง | ัก | หน | ให้ | ไม่ | หล | ่น | ึง | ▁แ | ทั | ตร | าร | ้อง | ไป | ิด | ข้า | ว่า | หม | คร | ือ | ล้ว | เป | เส | ประ | าน | ั่ง | ▁๏ | ▁ฯ | ที่ | อก | เล | ิน | ได | พล | ทร | ัด | นาง | ึก | ได้ | ู่ | ▁จะ | ค์ | ี้ | พร | เป็น | สุ | ทั้ง | อม | ัย | เร | ห็น | ▁จ | ▁พระ | ก็ | ใจ | อา | ื่ | ่าง | ต่ | กร | ิง | วง | วน | ือน | เจ | ู้ | ียง | อยู่ | รร | ตาม | ▁พ | ้วย | าว | ถึง | คล | ั้น | รี | เข | ด้วย | สม | องค์ | สน | าก | ▁แล้ว | เช | ัว | ย์ | ใน | คว | น้ | หมือน | ▁ส | ูก | อบ | กระ | เจ้า | ทรง | ลา | กัน | มี | ่าย | พรา | ิ่ง | เข้า | เห็น | ิต | สง | อด | ณ์ | วย | ้ม | คิด | เม | เก | เด | ▁นาง | วา | ุก | ▁ให้ | ดู | หา | ▁อ | ▁จึง | ทํา | ลง | รัก | เค | แล้ว | ่าน | พี่ | เหมือน | ั่น | ความ | ยง | อย่า | หร | มิ | ืน | ช่ | การ | ัญ | ▁ไม่ | ฝ่าย | ศรี | ้าง | วก | ้อม | ือง | น้อง | ยว | พา | แก | กํา | ่อน | ื่น | หน้า | ยา | ดี | ั้ง | ▁ทั้ง | ปรา | คน | เน | หว | รับ | แต่ | ้าย | ัส | เหล | ดา | สํา | นี้ | สาร | กับ | ลูก | ละ | ▁ต | รู้ | ื่อ | ▁ฝ่าย | ึ่ง | ลัง | าด | ื้ | กา | ขึ | นั่ง | เท | ▁เห็น | ฟัง | ้อย | ไร | ขึ้น | เสีย | ▁แต่ | บุ | สา | ไว | ทุก | กลับ | สุด | ัต | ใคร | น้ํา | ชา | ุด | ทัพ | วัน | สอง | นา | หย | ตา | รบ | ▁มา | ่อ | หรือ | ทู | ยัง | รง | จร | ปร | ▁บ | ไว้ | ดัง | วิ | ช่วย | ปล | ออก | ัตร | เพ | สิ | แจ | แล | ็จ | ิย์ | ▁พอ | มาร | ค่ | วรร | หมณ์ | คํา | เขา | นั้น | กษ | เย | ข้าง | หมา | เว | ไพร | หลัง | จิต | พราหมณ์ | ้ํา | ▁ถึง | ขอ | ทูล | สาม | ื้อ | วาย | อภ | ทาง | ▁แม | วัง | โฉ | ่ม | จน | ▁เป | ัตริย์ | ื่อง | สั่ง | แม่ | ▁ช | ฝ้า | โฉม | ราช | ฝร | ▁ถ | ฝรั่ง | ิ์ | ลม | แต | ▁เป็น | หาร | ื้น | เห | ้อน | ตาย | ุ่ง | ตัว | อย่าง | ลี | ผู้ | น้อย | ฉัน | ตรี | กุ | ษา | ุทร | ถาม | ของ | พร้อม | ชี | สร | เอ | ุง | พลาง | ตี | สมุทร | หาย | ที | วรรณ | เลี้ | นึก | จึง | หมาย | ▁ด้วย | ขว | ียน | ศึก | ่อง | ต้อง | ลัย | บา | พิ | อุ | สุวรรณ | โย | เรา | กลาง | เฝ้า | กษัตริย์ | สะ | แท | สัย | แจ้ง | หญ | ▁อย่า | รํา | ตรัส | อภัย | ผล | เลย | ียว | ไหน | ้าว | แน | ิดา | ริ | สาว | ิ้ม | เมือง | เล่า | ขัด | ค่อย | ภา | โอ | ่ํา | มัน | ชม | ห์ | ชาย | ัล | นาย | ▁เจ | เสียง | ยิ่ง | รู | ๋ย | เปล | เอา | ▁เส | คง | ตรา | ห้า | ินสมุทร | คอย | หญิง | หนี | ้าน | ญา | คุ | บรร | ▁ประ | กาย | ทหาร | ▁อัน | สิ้น | ทธ | ทอง | ักษ | ลังกา | นิ | พู | ศ์ | ่ว | จา | ใหญ | ที่ยว | มน | ไล | จริง | ▁เจ้า | จํา | ▁บ้าง | บอก | ▁ต่าง | ติ | ▁เข้า | ไม | ศร | อั | เคย | เลี้ยง | กรา | แสน | ▁จน | จับ | พบ | ครั้น | จง | พวก | สี | ไข | ษฐ | เกล | คา | รม | พัก | พัน | ซึ่ง | หนัก | นี | ่าว | กรุง | กล้ง | ▁เหมือน | ครา | เคร | ท้าว | ใส | ▁พวก | ตั้ง | หลง | ล้วน | ▁ไป | ผี | ลํา | นัก | ร้อง | ▁จง | ทรา | หนา | ▁ก็ | กลัว | ▁ที่ | เคียง | อาย | เรือ | ▁แม้น | เต | แค | ยก | พราะ | ใหญ่ | ▁ครั้น | ▁น | แก้ว | ถือ | ▁ได้ | เหลือ'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_vocabs = [pam_bpe_tokenizer.id_to_piece(id) for id in range(pam_bpe_tokenizer.get_piece_size())]\n",
    "\" | \".join(bpe_vocabs[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu6QnnRfQyFj"
   },
   "source": [
    "### User-defined symbols\n",
    "\n",
    "Another important concept to know of is User-defined symbols. These special symbols are reserved for a special purpose \\(e.g.\\, the \\<MASK\\> token used in BERT) and will always be tokenized into one token.\n",
    "\n",
    "Refer to the documentation for ways to add these special tokens to your tokenizer.\n",
    "\n",
    "https://github.com/google/sentencepiece/blob/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEFOj62ZEdzT"
   },
   "source": [
    "## Train another tokenizer on another domain\n",
    "\n",
    "Now try training another unigram tokenizer on `pantip_text` and we will use it to compare with the unigram tokenizer we trained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pantip_train_corpus = \"\\n\".join(pantip_train_text)\n",
    "pantip_test_corpus = \"\\n\".join(pantip_test_text)\n",
    "\n",
    "with open(\"pantip_train_corpus.txt\", \"w\") as f:\n",
    "    f.write(pantip_train_corpus)\n",
    "with open(\"pantip_test_corpus.txt\", \"w\") as f:\n",
    "    f.write(pantip_test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "O7-QkA1eMZFf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: pantip_train_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: pantip_unigram\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: pantip_train_corpus.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (7271 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 9475 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 20 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=795024\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9506% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=185\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999506\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 8875 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=344183\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 202602 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 8875\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 28564\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 28564 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=41593 obj=36.7304 num_tokens=117107 num_tokens/piece=2.81555\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=35185 obj=31.7493 num_tokens=118246 num_tokens/piece=3.36069\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=26169 obj=31.928 num_tokens=126191 num_tokens/piece=4.82216\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25868 obj=31.5765 num_tokens=126408 num_tokens/piece=4.88666\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19278 obj=32.403 num_tokens=135618 num_tokens/piece=7.03486\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19222 obj=32.081 num_tokens=135912 num_tokens/piece=7.07065\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14375 obj=33.0697 num_tokens=145771 num_tokens/piece=10.1406\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14365 obj=32.7542 num_tokens=145994 num_tokens/piece=10.1632\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10770 obj=33.8919 num_tokens=156025 num_tokens/piece=14.487\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10766 obj=33.5958 num_tokens=156169 num_tokens/piece=14.5058\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8071 obj=34.8705 num_tokens=166979 num_tokens/piece=20.6888\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8071 obj=34.5957 num_tokens=167077 num_tokens/piece=20.7009\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6053 obj=35.9697 num_tokens=178378 num_tokens/piece=29.4694\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6053 obj=35.6866 num_tokens=178906 num_tokens/piece=29.5566\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4538 obj=37.2058 num_tokens=190787 num_tokens/piece=42.0421\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4538 obj=36.9041 num_tokens=191139 num_tokens/piece=42.1197\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3403 obj=38.5838 num_tokens=204169 num_tokens/piece=59.9968\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3403 obj=38.2589 num_tokens=204177 num_tokens/piece=59.9991\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2552 obj=40.1566 num_tokens=218658 num_tokens/piece=85.681\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2552 obj=39.7848 num_tokens=218876 num_tokens/piece=85.7665\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1914 obj=41.8605 num_tokens=234410 num_tokens/piece=122.471\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1914 obj=41.4363 num_tokens=234410 num_tokens/piece=122.471\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1435 obj=43.7577 num_tokens=252316 num_tokens/piece=175.83\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1435 obj=43.2646 num_tokens=252333 num_tokens/piece=175.842\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=45.6164 num_tokens=272438 num_tokens/piece=247.671\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=45.0553 num_tokens=272438 num_tokens/piece=247.671\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: pantip_unigram.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: pantip_unigram.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: pantip_train_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: pantip_bpe\n",
      "  model_type: BPE\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: pantip_train_corpus.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (7271 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 9475 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 20 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=795024\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9506% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=185\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999506\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 8875 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 8875\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 28564\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7333 min_freq=114\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3173 size=20 all=5804 active=2376 piece=ื่\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2380 size=40 all=7671 active=4243 piece=ีย\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1715 size=60 all=9767 active=6339 piece=เร\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1319 size=80 all=11907 active=8479 piece=ค่ะ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1051 size=100 all=14341 active=10913 piece=่น\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1041 min_freq=116\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=894 size=120 all=16680 active=3222 piece=▁2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=763 size=140 all=19286 active=5828 piece=วิ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=659 size=160 all=21472 active=8014 piece=ราะ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=590 size=180 all=23971 active=10513 piece=ไหม\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=532 size=200 all=26267 active=12809 piece=▁จ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=531 min_freq=75\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=491 size=220 all=28595 active=3607 piece=an\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=449 size=240 all=30763 active=5775 piece=บ้าง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=403 size=260 all=32643 active=7655 piece=พอ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=368 size=280 all=35059 active=10071 piece=re\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=337 size=300 all=37122 active=12134 piece=คา\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=337 min_freq=45\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=316 size=320 all=38924 active=3561 piece=าะ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=299 size=340 all=40790 active=5427 piece=//\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=284 size=360 all=42844 active=7481 piece=ัส\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=273 size=380 all=44765 active=9402 piece=ttp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=260 size=400 all=46420 active=11057 piece=ขอ\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=260 min_freq=32\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=245 size=420 all=48202 active=3961 piece=รี\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=233 size=440 all=50161 active=5920 piece=ทํางาน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=220 size=460 all=51692 active=7451 piece=เดิน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=208 size=480 all=53265 active=9024 piece=กระท\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=197 size=500 all=55040 active=10799 piece=ic\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=195 min_freq=26\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=188 size=520 all=56667 active=4333 piece=เพิ่ม\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=180 size=540 all=58098 active=5764 piece=ตลอด\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=169 size=560 all=59824 active=7490 piece=ทุน\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=161 size=580 all=61407 active=9073 piece=te\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=156 size=600 all=62699 active=10365 piece=หลัก\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=156 min_freq=21\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=152 size=620 all=64239 active=4586 piece=วิต\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=148 size=640 all=65440 active=5787 piece=เร็\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=142 size=660 all=66481 active=6828 piece=▁9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=138 size=680 all=67726 active=8073 piece=สัย\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=133 size=700 all=69187 active=9534 piece=โทร\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=132 min_freq=19\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=128 size=720 all=70453 active=4622 piece=ส์\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=126 size=740 all=71615 active=5784 piece=▁ตอนนี้\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=122 size=760 all=72793 active=6962 piece=���ัง\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=119 size=780 all=73858 active=8027 piece=ข้อมูล\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=800 all=75243 active=9412 piece=คม\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=115 min_freq=17\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: pantip_bpe.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: pantip_bpe.vocab\n"
     ]
    }
   ],
   "source": [
    "## Train\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"pantip_train_corpus.txt\", model_prefix=\"pantip_unigram\", vocab_size=1000, model_type=\"unigram\"\n",
    ")\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"pantip_train_corpus.txt\", model_prefix=\"pantip_bpe\", vocab_size=1000, model_type=\"bpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5WOVMbONnYv"
   },
   "source": [
    "## Analyse top tokens on different datasets\n",
    "\n",
    "Use your tokenizers to tokenize the datasets and analyse your most common vocabularies (try 300-400 vocabs with len>1). Hint: tokenize your data and count the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wbfkGcsUrPYS"
   },
   "outputs": [],
   "source": [
    "pantip_unigram_tokenizer = spm.SentencePieceProcessor(model_file=\"pantip_unigram.model\")\n",
    "pantip_bpe_tokenizer = spm.SentencePieceProcessor(model_file=\"pantip_bpe.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_stats_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"source\",\n",
    "        \"tokenizer\",\n",
    "        \"token\",\n",
    "        \"count\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2371\n"
     ]
    }
   ],
   "source": [
    "# tokenizer trained on pantip -> tokenized on pantip\n",
    "unigram_token_counts = defaultdict(int)\n",
    "bpe_token_counts = defaultdict(int)\n",
    "for text in pantip_train_text + pantip_test_text:\n",
    "    # unigram\n",
    "    unigram_tokens = pantip_unigram_tokenizer.encode(text, out_type=str)\n",
    "    for token in unigram_tokens:\n",
    "        unigram_token_counts[token] += 1\n",
    "    # bpe\n",
    "    bpe_tokens = pantip_bpe_tokenizer.encode(text, out_type=str)\n",
    "    for token in bpe_tokens:\n",
    "        bpe_token_counts[token] += 1\n",
    "\n",
    "rows = []\n",
    "\n",
    "# unigram\n",
    "for token, count in unigram_token_counts.items():\n",
    "    rows.append(\n",
    "        {\n",
    "            \"source\": \"pantip\",\n",
    "            \"tokenizer\": \"pantip_unigram\",\n",
    "            \"token\": token,\n",
    "            \"count\": count,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# bpe\n",
    "for token, count in bpe_token_counts.items():\n",
    "    rows.append(\n",
    "        {\n",
    "            \"source\": \"pantip\",\n",
    "            \"tokenizer\": \"pantip_bpe\",\n",
    "            \"token\": token,\n",
    "            \"count\": count,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# append to the dataframe\n",
    "tokenization_stats_df = pd.concat([tokenization_stats_df, pd.DataFrame(rows)])\n",
    "\n",
    "print(f\"Number of rows: {len(tokenization_stats_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3750\n"
     ]
    }
   ],
   "source": [
    "# tokenizer trained on pantip -> tokenized on pam\n",
    "unigram_token_counts = defaultdict(int)\n",
    "bpe_token_counts = defaultdict(int)\n",
    "for text in pam_train_text + pam_test_text:\n",
    "    # unigram\n",
    "    unigram_tokens = pantip_unigram_tokenizer.encode(text, out_type=str)\n",
    "    for token in unigram_tokens:\n",
    "        unigram_token_counts[token] += 1\n",
    "    # bpe\n",
    "    bpe_tokens = pantip_bpe_tokenizer.encode(text, out_type=str)\n",
    "    for token in bpe_tokens:\n",
    "        bpe_token_counts[token] += 1\n",
    "\n",
    "rows = []\n",
    "\n",
    "# unigram\n",
    "for token, count in unigram_token_counts.items():\n",
    "    rows.append(\n",
    "        {\n",
    "            \"source\": \"pam\",\n",
    "            \"tokenizer\": \"pantip_unigram\",\n",
    "            \"token\": token,\n",
    "            \"count\": count,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# bpe\n",
    "for token, count in bpe_token_counts.items():\n",
    "    rows.append(\n",
    "        {\n",
    "            \"source\": \"pam\",\n",
    "            \"tokenizer\": \"pantip_bpe\",\n",
    "            \"token\": token,\n",
    "            \"count\": count,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# append to the dataframe\n",
    "tokenization_stats_df = pd.concat([tokenization_stats_df, pd.DataFrame(rows)])\n",
    "\n",
    "print(f\"Number of rows: {len(tokenization_stats_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 17810\n"
     ]
    }
   ],
   "source": [
    "# tokenizer trained on pam -> tokenized on pantip\n",
    "unigram_token_counts = defaultdict(int)\n",
    "bpe_token_counts = defaultdict(int)\n",
    "for text in pantip_train_text + pantip_test_text:\n",
    "    # unigram\n",
    "    unigram_tokens = pam_unigram_tokenizer.encode(text, out_type=str)\n",
    "    for token in unigram_tokens:\n",
    "        unigram_token_counts[token] += 1\n",
    "    # bpe\n",
    "    bpe_tokens = pam_bpe_tokenizer.encode(text, out_type=str)\n",
    "    for token in bpe_tokens:\n",
    "        bpe_token_counts[token] += 1\n",
    "\n",
    "rows = []\n",
    "\n",
    "# unigram\n",
    "for token, count in unigram_token_counts.items():\n",
    "    rows.append(\n",
    "        {\n",
    "            \"source\": \"pantip\",\n",
    "            \"tokenizer\": \"pam_unigram\",\n",
    "            \"token\": token,\n",
    "            \"count\": count,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# bpe\n",
    "for token, count in bpe_token_counts.items():\n",
    "    rows.append(\n",
    "        {\n",
    "            \"source\": \"pantip\",\n",
    "            \"tokenizer\": \"pam_bpe\",\n",
    "            \"token\": token,\n",
    "            \"count\": count,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# append to the dataframe\n",
    "tokenization_stats_df = pd.concat([tokenization_stats_df, pd.DataFrame(rows)])\n",
    "\n",
    "print(f\"Number of rows: {len(tokenization_stats_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 19806\n"
     ]
    }
   ],
   "source": [
    "# tokenizer trained on pam -> tokenized on pam\n",
    "unigram_token_counts = defaultdict(int)\n",
    "bpe_token_counts = defaultdict(int)\n",
    "for text in pam_train_text + pam_test_text:\n",
    "    # unigram\n",
    "    unigram_tokens = pam_unigram_tokenizer.encode(text, out_type=str)\n",
    "    for token in unigram_tokens:\n",
    "        unigram_token_counts[token] += 1\n",
    "    # bpe\n",
    "    bpe_tokens = pam_bpe_tokenizer.encode(text, out_type=str)\n",
    "    for token in bpe_tokens:\n",
    "        bpe_token_counts[token] += 1\n",
    "\n",
    "rows = []\n",
    "\n",
    "# unigram\n",
    "for token, count in unigram_token_counts.items():\n",
    "    rows.append(\n",
    "        {\n",
    "            \"source\": \"pam\",\n",
    "            \"tokenizer\": \"pam_unigram\",\n",
    "            \"token\": token,\n",
    "            \"count\": count,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# bpe\n",
    "for token, count in bpe_token_counts.items():\n",
    "    rows.append(\n",
    "        {\n",
    "            \"source\": \"pam\",\n",
    "            \"tokenizer\": \"pam_bpe\",\n",
    "            \"token\": token,\n",
    "            \"count\": count,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# append to the dataframe\n",
    "tokenization_stats_df = pd.concat([tokenization_stats_df, pd.DataFrame(rows)])\n",
    "\n",
    "print(f\"Number of rows: {len(tokenization_stats_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_stats_df.to_csv(\"tokenization_stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q duckdb\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>total_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pam</td>\n",
       "      <td>pam_bpe</td>\n",
       "      <td>445508.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pam</td>\n",
       "      <td>pam_unigram</td>\n",
       "      <td>446143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pam</td>\n",
       "      <td>pantip_bpe</td>\n",
       "      <td>577579.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pam</td>\n",
       "      <td>pantip_unigram</td>\n",
       "      <td>631336.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source       tokenizer  total_count\n",
       "0    pam         pam_bpe     445508.0\n",
       "1    pam     pam_unigram     446143.0\n",
       "2    pam      pantip_bpe     577579.0\n",
       "3    pam  pantip_unigram     631336.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\n",
    "    \"\"\"\n",
    "select source, tokenizer, sum(count) as total_count\n",
    "from tokenization_stats_df\n",
    "where source = 'pam'\n",
    "group by source, tokenizer\n",
    "order by source, total_count\n",
    "\"\"\"\n",
    ").to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>total_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pantip</td>\n",
       "      <td>pantip_unigram</td>\n",
       "      <td>442659.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pantip</td>\n",
       "      <td>pantip_bpe</td>\n",
       "      <td>448553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pantip</td>\n",
       "      <td>pam_bpe</td>\n",
       "      <td>474871.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pantip</td>\n",
       "      <td>pam_unigram</td>\n",
       "      <td>511583.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source       tokenizer  total_count\n",
       "0  pantip  pantip_unigram     442659.0\n",
       "1  pantip      pantip_bpe     448553.0\n",
       "2  pantip         pam_bpe     474871.0\n",
       "3  pantip     pam_unigram     511583.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.query(\n",
    "    \"\"\"\n",
    "select source, tokenizer, sum(count) as total_count\n",
    "from tokenization_stats_df\n",
    "where source = 'pantip'\n",
    "group by source, tokenizer\n",
    "order by source, total_count\n",
    "\"\"\"\n",
    ").to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5g/160h3py942nb4pbbgy0dhtz40000gn/T/ipykernel_60656/1760044563.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  pivot = tokenization_stats_df[(tokenization_stats_df['token'] != '▁') & tokenization_stats_df['token'].apply(lambda x : len(x) > 2)].pivot_table(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th colspan=\"4\" halign=\"left\">pam</th>\n",
       "      <th colspan=\"4\" halign=\"left\">pantip</th>\n",
       "      <th>diff_pam_bpe</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizer</th>\n",
       "      <th>pam_bpe</th>\n",
       "      <th>pam_unigram</th>\n",
       "      <th>pantip_bpe</th>\n",
       "      <th>pantip_unigram</th>\n",
       "      <th>pam_bpe</th>\n",
       "      <th>pam_unigram</th>\n",
       "      <th>pantip_bpe</th>\n",
       "      <th>pantip_unigram</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ทั้ง</th>\n",
       "      <td>1208</td>\n",
       "      <td>753</td>\n",
       "      <td>2025</td>\n",
       "      <td>2062</td>\n",
       "      <td>352</td>\n",
       "      <td>335</td>\n",
       "      <td>494</td>\n",
       "      <td>498</td>\n",
       "      <td>-817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>พระ</th>\n",
       "      <td>2191</td>\n",
       "      <td>2023</td>\n",
       "      <td>2829</td>\n",
       "      <td>4565</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>-638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ถึง</th>\n",
       "      <td>903</td>\n",
       "      <td>751</td>\n",
       "      <td>1427</td>\n",
       "      <td>1430</td>\n",
       "      <td>687</td>\n",
       "      <td>648</td>\n",
       "      <td>774</td>\n",
       "      <td>775</td>\n",
       "      <td>-524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ด้วย</th>\n",
       "      <td>963</td>\n",
       "      <td>963</td>\n",
       "      <td>1396</td>\n",
       "      <td>1396</td>\n",
       "      <td>861</td>\n",
       "      <td>863</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>-433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ได้</th>\n",
       "      <td>1406</td>\n",
       "      <td>1535</td>\n",
       "      <td>1808</td>\n",
       "      <td>2116</td>\n",
       "      <td>3488</td>\n",
       "      <td>3769</td>\n",
       "      <td>2885</td>\n",
       "      <td>3098</td>\n",
       "      <td>-402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ประ</th>\n",
       "      <td>1997</td>\n",
       "      <td>1182</td>\n",
       "      <td>2347</td>\n",
       "      <td>2199</td>\n",
       "      <td>1270</td>\n",
       "      <td>895</td>\n",
       "      <td>972</td>\n",
       "      <td>503</td>\n",
       "      <td>-350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ข้า</th>\n",
       "      <td>737</td>\n",
       "      <td>769</td>\n",
       "      <td>1046</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>186</td>\n",
       "      <td>257</td>\n",
       "      <td>0</td>\n",
       "      <td>-309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>นี้</th>\n",
       "      <td>774</td>\n",
       "      <td>945</td>\n",
       "      <td>1064</td>\n",
       "      <td>1064</td>\n",
       "      <td>2135</td>\n",
       "      <td>2551</td>\n",
       "      <td>1711</td>\n",
       "      <td>1629</td>\n",
       "      <td>-290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>แต่</th>\n",
       "      <td>775</td>\n",
       "      <td>818</td>\n",
       "      <td>1061</td>\n",
       "      <td>1022</td>\n",
       "      <td>967</td>\n",
       "      <td>982</td>\n",
       "      <td>1083</td>\n",
       "      <td>928</td>\n",
       "      <td>-286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>พี่</th>\n",
       "      <td>658</td>\n",
       "      <td>932</td>\n",
       "      <td>936</td>\n",
       "      <td>1191</td>\n",
       "      <td>282</td>\n",
       "      <td>367</td>\n",
       "      <td>304</td>\n",
       "      <td>369</td>\n",
       "      <td>-278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>กระ</th>\n",
       "      <td>887</td>\n",
       "      <td>864</td>\n",
       "      <td>1118</td>\n",
       "      <td>1233</td>\n",
       "      <td>484</td>\n",
       "      <td>615</td>\n",
       "      <td>345</td>\n",
       "      <td>427</td>\n",
       "      <td>-231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>รัก</th>\n",
       "      <td>839</td>\n",
       "      <td>930</td>\n",
       "      <td>1065</td>\n",
       "      <td>996</td>\n",
       "      <td>286</td>\n",
       "      <td>300</td>\n",
       "      <td>335</td>\n",
       "      <td>300</td>\n",
       "      <td>-226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ว่า</th>\n",
       "      <td>2445</td>\n",
       "      <td>2734</td>\n",
       "      <td>2663</td>\n",
       "      <td>2805</td>\n",
       "      <td>3586</td>\n",
       "      <td>4120</td>\n",
       "      <td>2701</td>\n",
       "      <td>2763</td>\n",
       "      <td>-218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ตาม</th>\n",
       "      <td>1232</td>\n",
       "      <td>1376</td>\n",
       "      <td>1444</td>\n",
       "      <td>1600</td>\n",
       "      <td>498</td>\n",
       "      <td>706</td>\n",
       "      <td>498</td>\n",
       "      <td>643</td>\n",
       "      <td>-212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>น้อง</th>\n",
       "      <td>736</td>\n",
       "      <td>968</td>\n",
       "      <td>846</td>\n",
       "      <td>1043</td>\n",
       "      <td>220</td>\n",
       "      <td>292</td>\n",
       "      <td>213</td>\n",
       "      <td>295</td>\n",
       "      <td>-110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ลูก</th>\n",
       "      <td>745</td>\n",
       "      <td>905</td>\n",
       "      <td>832</td>\n",
       "      <td>902</td>\n",
       "      <td>395</td>\n",
       "      <td>414</td>\n",
       "      <td>409</td>\n",
       "      <td>297</td>\n",
       "      <td>-87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>อยู่</th>\n",
       "      <td>1394</td>\n",
       "      <td>1653</td>\n",
       "      <td>1481</td>\n",
       "      <td>1543</td>\n",
       "      <td>1159</td>\n",
       "      <td>1292</td>\n",
       "      <td>1153</td>\n",
       "      <td>1154</td>\n",
       "      <td>-87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>รับ</th>\n",
       "      <td>828</td>\n",
       "      <td>786</td>\n",
       "      <td>883</td>\n",
       "      <td>903</td>\n",
       "      <td>797</td>\n",
       "      <td>2843</td>\n",
       "      <td>817</td>\n",
       "      <td>730</td>\n",
       "      <td>-55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>▁แต่</th>\n",
       "      <td>700</td>\n",
       "      <td>737</td>\n",
       "      <td>754</td>\n",
       "      <td>754</td>\n",
       "      <td>1333</td>\n",
       "      <td>1356</td>\n",
       "      <td>1367</td>\n",
       "      <td>1367</td>\n",
       "      <td>-54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>กับ</th>\n",
       "      <td>651</td>\n",
       "      <td>889</td>\n",
       "      <td>698</td>\n",
       "      <td>887</td>\n",
       "      <td>1270</td>\n",
       "      <td>1803</td>\n",
       "      <td>1336</td>\n",
       "      <td>1523</td>\n",
       "      <td>-47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "source        pam                                        pantip              \\\n",
       "tokenizer pam_bpe pam_unigram pantip_bpe pantip_unigram pam_bpe pam_unigram   \n",
       "token                                                                         \n",
       "ทั้ง         1208         753       2025           2062     352         335   \n",
       "พระ          2191        2023       2829           4565     144         144   \n",
       "ถึง           903         751       1427           1430     687         648   \n",
       "ด้วย          963         963       1396           1396     861         863   \n",
       "ได้          1406        1535       1808           2116    3488        3769   \n",
       "ประ          1997        1182       2347           2199    1270         895   \n",
       "ข้า           737         769       1046              0     175         186   \n",
       "นี้           774         945       1064           1064    2135        2551   \n",
       "แต่           775         818       1061           1022     967         982   \n",
       "พี่           658         932        936           1191     282         367   \n",
       "กระ           887         864       1118           1233     484         615   \n",
       "รัก           839         930       1065            996     286         300   \n",
       "ว่า          2445        2734       2663           2805    3586        4120   \n",
       "ตาม          1232        1376       1444           1600     498         706   \n",
       "น้อง          736         968        846           1043     220         292   \n",
       "ลูก           745         905        832            902     395         414   \n",
       "อยู่         1394        1653       1481           1543    1159        1292   \n",
       "รับ           828         786        883            903     797        2843   \n",
       "▁แต่          700         737        754            754    1333        1356   \n",
       "กับ           651         889        698            887    1270        1803   \n",
       "\n",
       "source                              diff_pam_bpe  \n",
       "tokenizer pantip_bpe pantip_unigram               \n",
       "token                                             \n",
       "ทั้ง             494            498         -817  \n",
       "พระ              174            174         -638  \n",
       "ถึง              774            775         -524  \n",
       "ด้วย             946            946         -433  \n",
       "ได้             2885           3098         -402  \n",
       "ประ              972            503         -350  \n",
       "ข้า              257              0         -309  \n",
       "นี้             1711           1629         -290  \n",
       "แต่             1083            928         -286  \n",
       "พี่              304            369         -278  \n",
       "กระ              345            427         -231  \n",
       "รัก              335            300         -226  \n",
       "ว่า             2701           2763         -218  \n",
       "ตาม              498            643         -212  \n",
       "น้อง             213            295         -110  \n",
       "ลูก              409            297          -87  \n",
       "อยู่            1153           1154          -87  \n",
       "รับ              817            730          -55  \n",
       "▁แต่            1367           1367          -54  \n",
       "กับ             1336           1523          -47  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pivot table for token and (source, tokenizer) the value is sum of count\n",
    "pivot = (\n",
    "    tokenization_stats_df[\n",
    "        (tokenization_stats_df[\"token\"] != \"▁\") & tokenization_stats_df[\"token\"].apply(lambda x: len(x) > 2)\n",
    "    ]\n",
    "    .pivot_table(index=[\"token\"], columns=[\"source\", \"tokenizer\"], values=\"count\", aggfunc=\"sum\", fill_value=0)\n",
    "    .sort_values((\"pam\", \"pam_unigram\"), ascending=False)\n",
    "    .head(50)\n",
    ")\n",
    "\n",
    "pivot[\"diff_pam_bpe\"] = pivot[(\"pam\", \"pam_bpe\")] - pivot[(\"pam\", \"pantip_bpe\")]\n",
    "# Word that same train\n",
    "pivot.sort_values(\"diff_pam_bpe\", ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz0GdZ-5YYM9"
   },
   "source": [
    "### To answer\n",
    "What are some notable differences you see between the two vocabs?\n",
    "\n",
    "Write your answer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QxxYr0QLbDoU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. the number of token that from the tokenizer trained on the same corpus is more compact(gives less tokens) than the tokenizer trained on the different corpus.\\n2. in PAM data, with Pantip tokenizer, the performance look worse if the word is from PAM domain. -> (พระ ข้า รัก กระ ถึง ด้วย)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. the number of token that from the tokenizer trained on the same corpus is more compact(gives less tokens) than the tokenizer trained on the different corpus.\n",
    "2. in PAM data, with Pantip tokenizer, the performance look worse if the word is from PAM domain. -> (พระ ข้า รัก กระ ถึง ด้วย)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipjO87HPYl4N"
   },
   "source": [
    "## Using tokenizer across domains\n",
    "\n",
    "One problem you may face is your dataset is very specialized. In that case the tokenizer trained on a general domain may not perform as good as it should when used on your dataset.\n",
    "\n",
    "Next you will try using tokenizers trained on one general domain (on Pantip) and use it on a specialized domain (พระอภัยมณี) and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3tCh1RaZrTAM"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>total_count</th>\n",
       "      <th>tokenizer_train_corpus</th>\n",
       "      <th>tokenizer_algo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pam</td>\n",
       "      <td>pam_bpe</td>\n",
       "      <td>445508.0</td>\n",
       "      <td>pam</td>\n",
       "      <td>bpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pam</td>\n",
       "      <td>pam_unigram</td>\n",
       "      <td>446143.0</td>\n",
       "      <td>pam</td>\n",
       "      <td>unigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pam</td>\n",
       "      <td>pantip_bpe</td>\n",
       "      <td>577579.0</td>\n",
       "      <td>pantip</td>\n",
       "      <td>bpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pam</td>\n",
       "      <td>pantip_unigram</td>\n",
       "      <td>631336.0</td>\n",
       "      <td>pantip</td>\n",
       "      <td>unigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pantip</td>\n",
       "      <td>pantip_unigram</td>\n",
       "      <td>442659.0</td>\n",
       "      <td>pantip</td>\n",
       "      <td>unigram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pantip</td>\n",
       "      <td>pantip_bpe</td>\n",
       "      <td>448553.0</td>\n",
       "      <td>pantip</td>\n",
       "      <td>bpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pantip</td>\n",
       "      <td>pam_bpe</td>\n",
       "      <td>474871.0</td>\n",
       "      <td>pam</td>\n",
       "      <td>bpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pantip</td>\n",
       "      <td>pam_unigram</td>\n",
       "      <td>511583.0</td>\n",
       "      <td>pam</td>\n",
       "      <td>unigram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source       tokenizer  total_count tokenizer_train_corpus tokenizer_algo\n",
       "0     pam         pam_bpe     445508.0                    pam            bpe\n",
       "1     pam     pam_unigram     446143.0                    pam        unigram\n",
       "2     pam      pantip_bpe     577579.0                 pantip            bpe\n",
       "3     pam  pantip_unigram     631336.0                 pantip        unigram\n",
       "4  pantip  pantip_unigram     442659.0                 pantip        unigram\n",
       "5  pantip      pantip_bpe     448553.0                 pantip            bpe\n",
       "6  pantip         pam_bpe     474871.0                    pam            bpe\n",
       "7  pantip     pam_unigram     511583.0                    pam        unigram"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_by_source_and_tokenizer_df = duckdb.query(\n",
    "    \"\"\"\n",
    "select source, tokenizer, sum(count) as total_count\n",
    "from tokenization_stats_df\n",
    "group by source, tokenizer\n",
    "order by source, total_count\n",
    "\"\"\"\n",
    ").to_df()\n",
    "count_by_source_and_tokenizer_df[\"tokenizer_train_corpus\"] = count_by_source_and_tokenizer_df[\"tokenizer\"].apply(\n",
    "    lambda x: x.split(\"_\")[0]\n",
    ")\n",
    "count_by_source_and_tokenizer_df[\"tokenizer_algo\"] = count_by_source_and_tokenizer_df[\"tokenizer\"].apply(\n",
    "    lambda x: x.split(\"_\")[1]\n",
    ")\n",
    "\n",
    "count_by_source_and_tokenizer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4_6JG_l5BXh"
   },
   "source": [
    "### Q3 MCV\n",
    "\n",
    "What percentage increase do you observe when tokenizing the whole พระอภัยมณี dataset with a tokenizer trained on Pantip compared to the one trained on พระอภัยมณี."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token from PAM\n",
      "Token on the same domain: 445508.0\n",
      "Token on the different domain: 577579.0\n",
      "Increase: 29.65%\n"
     ]
    }
   ],
   "source": [
    "token_on_same_domain = (\n",
    "    duckdb.query(\n",
    "        \"\"\"\n",
    "select total_count\n",
    "from count_by_source_and_tokenizer_df\n",
    "where source = 'pam' and tokenizer = 'pam_bpe'\n",
    "\"\"\"\n",
    "    )\n",
    "    .to_df()\n",
    "    .values[0][0]\n",
    ")\n",
    "token_on_different_domain = (\n",
    "    duckdb.query(\n",
    "        \"\"\"\n",
    "select total_count\n",
    "from count_by_source_and_tokenizer_df\n",
    "where source = 'pam' and tokenizer = 'pantip_bpe'\n",
    "\"\"\"\n",
    "    )\n",
    "    .to_df()\n",
    "    .values[0][0]\n",
    ")\n",
    "\n",
    "print(f\"Token from PAM\")\n",
    "print(f\"Token on the same domain: {token_on_same_domain}\")\n",
    "print(f\"Token on the different domain: {token_on_different_domain}\")\n",
    "print(f\"Increase: {(token_on_different_domain - token_on_same_domain) / token_on_same_domain * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duaCJRO96SX1"
   },
   "source": [
    "### Q4 MCV\n",
    "\n",
    "What percentage increase do you observe when tokenizing the whole Pantip dataset with a tokenizer trained on พระอภัยมณี compared to the one trained on Pantip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "axk9gOIgrTYd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token from Pantip data\n",
      "Token on the same domain: 448553.0\n",
      "Token on the different domain: 474871.0\n",
      "Increase: 5.87%\n"
     ]
    }
   ],
   "source": [
    "token_on_same_domain = (\n",
    "    duckdb.query(\n",
    "        \"\"\"\n",
    "select total_count\n",
    "from count_by_source_and_tokenizer_df\n",
    "where source = 'pantip' and tokenizer = 'pantip_bpe'\n",
    "\"\"\"\n",
    "    )\n",
    "    .to_df()\n",
    "    .values[0][0]\n",
    ")\n",
    "\n",
    "token_on_different_domain = (\n",
    "    duckdb.query(\n",
    "        \"\"\"\n",
    "select total_count\n",
    "from count_by_source_and_tokenizer_df\n",
    "where source = 'pantip' and tokenizer = 'pam_bpe'\n",
    "\"\"\"\n",
    "    )\n",
    "    .to_df()\n",
    "    .values[0][0]\n",
    ")\n",
    "\n",
    "print(\"Token from Pantip data\")\n",
    "print(f\"Token on the same domain: {token_on_same_domain}\")\n",
    "print(f\"Token on the different domain: {token_on_different_domain}\")\n",
    "print(f\"Increase: {(token_on_different_domain - token_on_same_domain) / token_on_same_domain * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZYKuamv7-wI"
   },
   "source": [
    "### To answer\n",
    "Why do you think the number of tokens tokenized by the general tokenizer (the one trained on Pantip) has a higher percentage increase compared to the number of tokens tokenized by the specialized tokenizer? (Hint: we fixed vocab size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "gh9a6d7Q8ivJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBecause we fix top 1000 vocab size and word that exist on PAM data(พระอภัยมณี) is old period word. \\nThen, if we use PAM tokenizer to tokenize Pantip data, the performance is not growth as much as we use Pantip tokenizer to tokenize PAM data.\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Because we fix top 1000 vocab size and word that exist on PAM data(พระอภัยมณี) is old period word. \n",
    "Then, if we use PAM tokenizer to tokenize Pantip data, the performance is not growth as much as we use Pantip tokenizer to tokenize PAM data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7j_Cc0p9-5S"
   },
   "source": [
    "## The effect on language models\n",
    "\n",
    "Next, we will see the effect of using \"cross-domain\" tokenizers on Language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiWztANvohhn"
   },
   "source": [
    "### Setup\n",
    "We are going to reuse the code from the last assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "7pVtSbmVpwOo"
   },
   "outputs": [],
   "source": [
    "!pip install -q lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "JMt5GzLrW4x3"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import lightning as L\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "0OIs_VS_oo1M"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, seq_len=128):\n",
    "        token_ids = [tokenizer.encode(d, add_bos=True, add_eos=True) for d in data]\n",
    "        flatten_token_ids = list(itertools.chain(*token_ids))\n",
    "        encoded = torch.LongTensor(flatten_token_ids)\n",
    "\n",
    "        left_over = len(encoded) % seq_len\n",
    "        encoded = encoded[: len(encoded) - left_over]\n",
    "        self.encoded = encoded.view(-1, seq_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "hk6vEPiMq34n"
   },
   "outputs": [],
   "source": [
    "class LSTM(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        output = self.fc(lstm_out)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        prediction = self(src)\n",
    "        prediction = prediction.reshape(-1, self.vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        with torch.no_grad():\n",
    "            prediction = self(src)\n",
    "        prediction = prediction.reshape(-1, self.vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "oKhuOygixndB"
   },
   "outputs": [],
   "source": [
    "vocab_size = pam_unigram_tokenizer.get_piece_size()\n",
    "embedding_dim = 200\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "dropout_rate = 0.2\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_batch_size = 64\n",
    "test_batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOtOE7mr-heY"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8-x9HiPDcpE"
   },
   "source": [
    "<a name=\"no1\"></a>\n",
    "#### 1. Training on Pantip data with Pantip tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "oUv_A4MTx0Ob"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 200 K  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 513 K  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.511    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 44/44 [00:08<00:00,  5.17it/s, v_num=4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 44/44 [00:08<00:00,  5.05it/s, v_num=4]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=10, deterministic=True)\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, pantip_unigram_tokenizer)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, pantip_unigram_tokenizer)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, pantip_unigram_tokenizer)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, pantip_unigram_tokenizer)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pantip_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "1e-Y1_GYy65g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 3: 100%|██████████| 9/9 [00:01<00:00,  8.02it/s]  \n",
      "Perplexity on Pantip train set is:\t39.45305689850137\n",
      "Perplexity on Pra apai manee train set is:\t107.59757736986049\n",
      "Perplexity on Pantip test set is:\t142.8758471391891\n",
      "Perplexity on Pra apai manee test set is:\t146.00030674317318\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(\n",
    "    model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader, pam_test_loader], verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s3AmE4nDjmL"
   },
   "source": [
    "<a name=\"no2\"></a>\n",
    "#### 2. Training on Pantip data with Pra apai manee tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "vfRdW3m1Dmj_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/44 [04:38<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/44 [03:50<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/44 [02:34<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 200 K  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 513 K  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.511    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: 100%|██████████| 51/51 [00:09<00:00,  5.20it/s, v_num=5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 51/51 [00:10<00:00,  5.07it/s, v_num=5]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=10, deterministic=True)\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, pam_unigram_tokenizer)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, pam_unigram_tokenizer)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, pam_unigram_tokenizer)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, pam_unigram_tokenizer)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pantip_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "xwLN1IarD3g9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 3: 100%|██████████| 7/7 [00:00<00:00,  7.56it/s]  \n",
      "Perplexity on Pantip train set is:\t19.768709152369343\n",
      "Perplexity on Pra apai manee train set is:\t455.0351549376166\n",
      "Perplexity on Pantip test set is:\t50.90800060010585\n",
      "Perplexity on Pra apai manee test set is:\t377.87146572784667\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(\n",
    "    model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader, pam_test_loader], verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB8zqptTWcA6"
   },
   "source": [
    "#### To answer\n",
    "\n",
    "The perplexity numbers should indicate that:\n",
    "1. Training the LM with Pra apai manee tokenizer on Pantip (no. [2](#no2)) results in overfitting to Pantip and poor generalization to the Pra apai manee dataset.\n",
    "2. However using the Pantip tokenizer (no. [1](#no1)) results in a much better generalization.\n",
    "\n",
    "Try and come up with some reasons for the results above. <br>\n",
    "Hint:\n",
    "1. think about \"general\" vocabs and domain-specific vocabs.\n",
    "2. what do you think happens to the model when the token ids become longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmHGQf2saPj_"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PAM tokenizer’s vocabulary is less general and modern compared to Pantip’s. \n",
    "It relies on longer tokens, requiring more specific subwords (e.g., s## is more generalized than se##). \n",
    "This causes the model’s attention to be distributed across a wider variety of tokens, making it harder to generalize and learn dependencies between them, \n",
    "which is a limitation of the PAM tokenizer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8VPMm7pLdSl"
   },
   "source": [
    "\n",
    "<a name=\"no3\"></a>\n",
    "#### 3. Training on Pra apai manee data with Pantip tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "oR5fp-YCLnnU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 200 K  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 513 K  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.511    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 66/66 [00:13<00:00,  5.06it/s, v_num=6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 66/66 [00:13<00:00,  4.99it/s, v_num=6]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=10, deterministic=True)\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, pantip_unigram_tokenizer)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, pantip_unigram_tokenizer)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, pantip_unigram_tokenizer)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, pantip_unigram_tokenizer)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pam_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "f_LhF7w7Lxwo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 3: 100%|██████████| 9/9 [00:01<00:00,  8.20it/s]  \n",
      "Perplexity on Pantip train set is:\t2904.773227256836\n",
      "Perplexity on Pra apai manee train set is:\t24.55698071112975\n",
      "Perplexity on Pantip test set is:\t1152.3588205405729\n",
      "Perplexity on Pra apai manee test set is:\t36.7126200093089\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(\n",
    "    model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader, pam_test_loader], verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apk9crJjMLoW"
   },
   "source": [
    "<a name=\"no4\"></a>\n",
    "#### 4. Training on Pra apai manee data with Pra apai manee tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "_G7GMBIKLzGK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 200 K  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 513 K  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.511    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 48/48 [00:09<00:00,  4.82it/s, v_num=7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 48/48 [00:10<00:00,  4.74it/s, v_num=7]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=10, deterministic=True)\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, pam_unigram_tokenizer)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, pam_unigram_tokenizer)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, pam_unigram_tokenizer)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, pam_unigram_tokenizer)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pam_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "9H753o_JMRFw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 3: 100%|██████████| 7/7 [00:00<00:00,  8.06it/s]  \n",
      "Perplexity on Pantip train set is:\t369.715941589192\n",
      "Perplexity on Pra apai manee train set is:\t77.35313006676817\n",
      "Perplexity on Pantip test set is:\t286.4953979845209\n",
      "Perplexity on Pra apai manee test set is:\t103.6682372873689\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.test(\n",
    "    model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader, pam_test_loader], verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9Lmmj4dZ-1"
   },
   "source": [
    "#### To answer\n",
    "\n",
    "The perplexity numbers should indicate that:\n",
    "1. Both LM overfits on Pra apai manee data and performs really bad on Pantip data.\n",
    "2. However using the Pra apai manee tokenizer (no. [4](#no4)) results in a  better generalization than the Pantip tokenizer(no. [3](#no3)).\n",
    "\n",
    "Try and come up with some reasons for the results above. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "HlE-mWSMfbv3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nUsing “PAM data with the PAM tokenizer” performs better, possibly because:\\n\\nThe tokenizer’s vocabulary is better aligned with the training data, unlike in scenario 3. \\nThe PAM tokenizer is specifically designed for the linguistic patterns in PAM data (e.g., poems, which differ from the conversational style of Pantip data). \\nThis enables the model to learn more effective representations for these tokens, allowing it to generalize slightly better to unseen domains like Pantip data.\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Using “PAM data with the PAM tokenizer” performs better, possibly because:\n",
    "\n",
    "The tokenizer’s vocabulary is better aligned with the training data, unlike in scenario 3. \n",
    "The PAM tokenizer is specifically designed for the linguistic patterns in PAM data (e.g., poems, which differ from the conversational style of Pantip data). \n",
    "This enables the model to learn more effective representations for these tokens, allowing it to generalize slightly better to unseen domains like Pantip data.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
