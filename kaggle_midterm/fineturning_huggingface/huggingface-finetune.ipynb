{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a8941",
   "metadata": {
    "id": "d12a8941",
    "papermill": {
     "duration": 25.945716,
     "end_time": "2025-03-01T09:47:08.574265",
     "exception": false,
     "start_time": "2025-03-01T09:46:42.628549",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Import learning rate scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import LightningModule\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ebd66a",
   "metadata": {
    "id": "35ebd66a",
    "papermill": {
     "duration": 0.00222,
     "end_time": "2025-03-01T09:47:08.579471",
     "exception": false,
     "start_time": "2025-03-01T09:47:08.577251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1f769",
   "metadata": {
    "id": "1ab1f769",
    "papermill": {
     "duration": 0.007866,
     "end_time": "2025-03-01T09:47:08.589584",
     "exception": false,
     "start_time": "2025-03-01T09:47:08.581718",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "K_FOLDS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "INPUT_SIZE = 768 * 2\n",
    "LAYER_NUM = 7\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "LEARNING_RATE = 1e-2\n",
    "WEIGHT_DECAY = 1e-2\n",
    "\n",
    "# Learning rate scheduler\n",
    "LRS_PATIENCE = 10\n",
    "LRS_FACTOR = 0.5\n",
    "\n",
    "# Early stopping\n",
    "ES_PATIENCE = 50\n",
    "\n",
    "MAX_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d498e0",
   "metadata": {
    "id": "a1d498e0",
    "papermill": {
     "duration": 0.073628,
     "end_time": "2025-03-01T09:47:08.665624",
     "exception": false,
     "start_time": "2025-03-01T09:47:08.591996",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_df = pd.read_csv(\"/kaggle/input/nlp-2025-midterm-kaggle-asas/train.csv\", index_col=\"ID\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0cf19f",
   "metadata": {
    "id": "4c0cf19f",
    "papermill": {
     "duration": 0.012085,
     "end_time": "2025-03-01T09:47:08.680656",
     "exception": false,
     "start_time": "2025-03-01T09:47:08.668571",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Assign fold to each row\n",
    "train_df[\"fold\"] = -1\n",
    "for fold, (train_idx, val_idx) in enumerate(KFold(n_splits=K_FOLDS, shuffle=True, random_state=42).split(train_df)):\n",
    "    train_df.loc[val_idx, \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda545f",
   "metadata": {
    "id": "5cda545f",
    "papermill": {
     "duration": 0.0089,
     "end_time": "2025-03-01T09:47:08.692254",
     "exception": false,
     "start_time": "2025-03-01T09:47:08.683354",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ParaphraseMMPNETDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        has_score: bool = True,\n",
    "    ) -> None:\n",
    "        self.df = df\n",
    "\n",
    "        self.has_score = has_score\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        model_inputs = {\"question\": row[\"question\"], \"answer\": row[\"answer\"]}\n",
    "\n",
    "        # Get the embeddings\n",
    "        if self.has_score:\n",
    "            return model_inputs, row[\"score\"].astype(np.float32)\n",
    "        else:\n",
    "            return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf7dfa",
   "metadata": {
    "id": "5eaf7dfa",
    "papermill": {
     "duration": 0.012725,
     "end_time": "2025-03-01T09:47:08.707595",
     "exception": false,
     "start_time": "2025-03-01T09:47:08.694870",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLPRegressor(LightningModule):\n",
    "    def __init__(self, input_size: int, layer_num: int, dropout_rate: float) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size // 2),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        for _ in range(layer_num):\n",
    "            self.model.add_module(f\"linear_{_}\", nn.Linear(input_size // 2, input_size // 2))\n",
    "            self.model.add_module(f\"relu_{_}\", nn.LeakyReLU())\n",
    "            self.model.add_module(f\"dropout_{_}\", nn.Dropout(dropout_rate))\n",
    "            if _ % 3 == 0:\n",
    "                self.model.add_module(f\"batchnorm_{_}\", nn.BatchNorm1d(input_size // 2))\n",
    "        self.model.add_module(\"output\", nn.Linear(input_size // 2, 1))\n",
    "        self.model.add_module(\"sigmoid\", nn.Sigmoid())\n",
    "\n",
    "        self.embedder_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.model.apply(self._init_weights)\n",
    "\n",
    "        self.loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    def _init_weights(self, layer):\n",
    "        if type(layer) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "            torch.nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        question_embeds = self.embedder_model.encode(x[\"question\"], show_progress_bar=False)\n",
    "        answer_embeds = self.embedder_model.encode(x[\"answer\"], show_progress_bar=False)\n",
    "        embeddings = np.concatenate([question_embeds, answer_embeds], axis=-1)\n",
    "        return self.model(torch.tensor(embeddings, device=self.device)) * 5.0\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # Log the learning rate\n",
    "        self.log(\n",
    "            \"learning_rate\",\n",
    "            self.trainer.optimizers[0].param_groups[0][\"lr\"],\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=LRS_FACTOR, patience=LRS_PATIENCE, verbose=True)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80997e",
   "metadata": {
    "id": "6b80997e",
    "papermill": {
     "duration": 1995.655079,
     "end_time": "2025-03-01T10:20:24.365244",
     "exception": false,
     "start_time": "2025-03-01T09:47:08.710165",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainers = []\n",
    "for current_fold in range(K_FOLDS):\n",
    "    model = MLPRegressor(input_size=INPUT_SIZE, layer_num=LAYER_NUM, dropout_rate=DROPOUT_RATE)\n",
    "\n",
    "    train_fold_df = train_df[train_df[\"fold\"] != current_fold]\n",
    "    val_fold_df = train_df[train_df[\"fold\"] == current_fold]\n",
    "\n",
    "    # Create the datasets and dataloaders\n",
    "    train_dataset = ParaphraseMMPNETDataset(train_fold_df)\n",
    "    val_dataset = ParaphraseMMPNETDataset(val_fold_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
    "\n",
    "    # Create logger\n",
    "    logger = CSVLogger(\n",
    "        \"logs/paraphrase-multilingual-mpnet-base-v2\", name=f\"paraphrase-multilingual-mpnet-base-v2-{current_fold}\"\n",
    "    )\n",
    "\n",
    "    # Create a checkpoint callback\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=f\"./logs/paraphrase-multilingual-mpnet-base-v2/paraphrase-multilingual-mpnet-base-v2-{current_fold}/checkpoints\",\n",
    "        filename=\"paraphrase-multilingual-mpnet-base-v2-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    # Create an early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=ES_PATIENCE, mode=\"min\")\n",
    "\n",
    "    # Create a trainer\n",
    "    trainer = Trainer(max_epochs=MAX_EPOCHS, logger=logger, callbacks=[checkpoint_callback, early_stopping])\n",
    "\n",
    "    # Fit the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # Append the trainer\n",
    "    trainers.append(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463ebfe",
   "metadata": {
    "id": "b463ebfe",
    "papermill": {
     "duration": 43.588164,
     "end_time": "2025-03-01T10:21:08.064837",
     "exception": false,
     "start_time": "2025-03-01T10:20:24.476673",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_models = []\n",
    "for current_fold in range(K_FOLDS):\n",
    "    model = MLPRegressor.load_from_checkpoint(\n",
    "        trainers[current_fold].checkpoint_callback.best_model_path,\n",
    "        input_size=INPUT_SIZE,\n",
    "        layer_num=LAYER_NUM,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "    )\n",
    "    best_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa4b49",
   "metadata": {
    "id": "09aa4b49",
    "papermill": {
     "duration": 0.055402,
     "end_time": "2025-03-01T10:21:08.177299",
     "exception": false,
     "start_time": "2025-03-01T10:21:08.121897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d2383",
   "metadata": {
    "id": "c56d2383",
    "papermill": {
     "duration": 4.907912,
     "end_time": "2025-03-01T10:21:13.141435",
     "exception": false,
     "start_time": "2025-03-01T10:21:08.233523",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/nlp-2025-midterm-kaggle-asas/test.csv\", index_col=\"ID\")\n",
    "\n",
    "test_dataset = ParaphraseMMPNETDataset(test_df, has_score=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "predictions = []\n",
    "for model in best_models:\n",
    "    model.eval()\n",
    "    model.freeze()\n",
    "\n",
    "    model_predictions = []\n",
    "    for x in test_loader:\n",
    "        y_hat = model(x).squeeze()\n",
    "        model_predictions.extend(y_hat.tolist())\n",
    "\n",
    "    predictions.append(model_predictions)\n",
    "\n",
    "# Average the predictions\n",
    "predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "test_df[\"score\"] = predictions\n",
    "test_df[[\"score\"]].to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11180927,
     "sourceId": 93859,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2076.731941,
   "end_time": "2025-03-01T10:21:16.750114",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-01T09:46:40.018173",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
