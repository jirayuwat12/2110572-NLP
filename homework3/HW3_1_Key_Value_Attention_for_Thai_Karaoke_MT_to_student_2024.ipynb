{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfUmXr1D1ZSR"
   },
   "source": [
    "# Key-Value Attention for Thai Karaoke Character-level Machine Translation (Many-to-Many, encoder-decoder)\n",
    "\n",
    "In this homework, you will create an MT model with attention mechnism that coverts names of Thai 2019 MP candidates from Thai script to Roman(Latin) script. E.g. นิยม-->niyom\n",
    "\n",
    "The use of Pytorch Lightning is optional but recommended. You can use Pytorch if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "18KMSkqZ-Pt-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install lightning wandb\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/Phonbopit/sarabun-webfont/master/fonts/thsarabunnew-webfont.ttf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKCBCWKARZEx",
    "outputId": "6ad8a585-fd68-44ee-deac-37143137cbcc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjirayuwat12\u001b[0m (\u001b[33mmyfistteam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/jirayuwat/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"648f0ebca50c7021eefe306ab62fcbf0029574da\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ka2TN8IV1ZSU"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.font_manager.fontManager.addfont(\"thsarabunnew-webfont.ttf\")  # 3.2+\n",
    "mpl.rc(\"font\", family=\"TH Sarabun New\")\n",
    "import torch\n",
    "\n",
    "# import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-f_s6vX1ZSZ"
   },
   "source": [
    "## Load Dataset\n",
    "We have generated a toy dataset using names of Thai MP candidates in 2019 Thai General Election from elect.in.th's github(https://github.com/codeforthailand/dataset-election-62-candidates) and tltk (https://pypi.org/project/tltk/) library to convert them into Roman script.\n",
    "\n",
    "```\n",
    "ไกรสีห์ kraisi\n",
    "พัชรี phatri\n",
    "ธีระ thira\n",
    "วุฒิกร wutthikon\n",
    "ไสว sawai\n",
    "สัมภาษณ์  samphat\n",
    "วศิน wasin\n",
    "ทินวัฒน์ thinwat\n",
    "ศักดินัย sakdinai\n",
    "สุรศักดิ์ surasak\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jte-Csrf-4kd",
    "outputId": "a1ba364b-64c2-4875-873c-90bc1808a4be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-23 20:11:47--  https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/mp_name_th_en.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 324399 (317K) [text/plain]\n",
      "Saving to: ‘mp_name_th_en.csv.1’\n",
      "\n",
      "mp_name_th_en.csv.1 100%[===================>] 316.80K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-01-23 20:11:47 (9.12 MB/s) - ‘mp_name_th_en.csv.1’ saved [324399/324399]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/mp_name_th_en.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "L9zXp7KH1ZSa"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"mp_name_th_en.csv\") as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=\",\")\n",
    "    name_th = []\n",
    "    name_en = []\n",
    "    for row in readCSV:\n",
    "        temp_th = row[0]\n",
    "        temp_en = row[1]\n",
    "\n",
    "        name_th.append(temp_th)\n",
    "        name_en.append(temp_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZCsqrXxu1ZSe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ไกรสีห์ kraisi\n",
      "พัชรี phatri\n",
      "ธีระ thira\n",
      "วุฒิกร wutthikon\n",
      "ไสว sawai\n",
      "สัมภาษณ์  samphat\n",
      "วศิน wasin\n",
      "ทินวัฒน์ thinwat\n",
      "ศักดินัย sakdinai\n",
      "สุรศักดิ์ surasak\n"
     ]
    }
   ],
   "source": [
    "for th, en in zip(name_th[:10], name_en[:10]):\n",
    "    print(th, en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvW8xqT81ZSh"
   },
   "source": [
    "## TODO1: Preprocess dataset\n",
    "* You will need 2 vocabularies (1 for input and another for output)\n",
    "* DON'T FORGET TO INCLUDE special token for padding (for both input and output)\n",
    "* DON'T FORGET TO INCLUDE special token for the end of word symbol (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rv1Xd9A1ZSi",
    "outputId": "dece74ae-a492-41a7-f07d-2798157c7fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10887 lines and 65 unique characters in your input data.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "input_chars = list(set(\"\".join(name_th)))\n",
    "output_chars = list(set(\"\".join(name_en)))\n",
    "data_size, vocab_size = len(name_th), len(input_chars) + 1\n",
    "output_vocab_size = len(output_chars) + 2  # +2 for special end of sentence token/PADDING\n",
    "print(\"There are %d lines and %d unique characters in your input data.\" % (data_size, vocab_size))\n",
    "maxlen = len(max(name_th, key=len))  # max input length\n",
    "maxlen_out = len(max(name_en, key=len))  # max input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mo381I_t1ZSm",
    "outputId": "4467516a-90c8-477d-bc43-bb071b17a956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max input length: 20\n",
      "Max output length: 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Max input length:\", maxlen)\n",
    "print(\"Max output length:\", maxlen_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 65\n",
      "Output vocab size: 24\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from character to unique index.\n",
    "input_vocab = [\"<PAD>\"] + sorted(input_chars)\n",
    "input_char_to_idx = {char: idx for idx, char in enumerate(input_vocab)}\n",
    "input_idx_to_char = {idx: char for char, idx in input_char_to_idx.items()}\n",
    "print(f\"Input vocab size: {len(input_vocab)}\")\n",
    "\n",
    "output_vocab = [\"<PAD>\", \"</s>\"] + sorted(output_chars)\n",
    "output_char_to_idx = {char: idx for idx, char in enumerate(output_vocab)}\n",
    "output_idx_to_char = {idx: char for char, idx in output_char_to_idx.items()}\n",
    "print(f\"Output vocab size: {len(output_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "X = []\n",
    "y = []\n",
    "for data_index in range(data_size):\n",
    "    X.append(\n",
    "        [input_char_to_idx[char] for char in name_th[data_index]]\n",
    "        + [input_char_to_idx[\"<PAD>\"]] * (maxlen - len(name_th[data_index]))\n",
    "    )\n",
    "    y.append(\n",
    "        [output_char_to_idx[char] for char in name_en[data_index]]\n",
    "        + [output_char_to_idx[\"</s>\"]]\n",
    "        + [output_char_to_idx[\"<PAD>\"]] * (maxlen_out - len(name_en[data_index]))\n",
    "    ) \n",
    "\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "W3aXyJBEC-j_"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-yirzlseC9NS"
   },
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, X: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qUPAB7LTDFOy"
   },
   "outputs": [],
   "source": [
    "class NameDataModule(L.LightningDataModule):\n",
    "    def __init__(self, train_data, y, batch_size, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        X, y = zip(*batch)\n",
    "        # One hot encode\n",
    "        X = F.one_hot(torch.stack(X), num_classes=len(input_char_to_idx)).float()\n",
    "        y = torch.stack(y)\n",
    "        return X, y\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = NameDataset(self.train_data, self.y)\n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 65]) torch.Size([32, 20])\n"
     ]
    }
   ],
   "source": [
    "data_module = NameDataModule(X, y, batch_size=32, num_workers=0)\n",
    "\n",
    "data_loader = data_module.train_dataloader()\n",
    "for X_batch, y_batch in data_loader:\n",
    "    print(X_batch.shape, y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFSG1FqK1ZSy"
   },
   "source": [
    "# Attention Mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAlOrhbismQp"
   },
   "source": [
    "## TODO 2: Code your own (key-value) attention mechnism\n",
    "* PLEASE READ: you DO NOT have to follow all the details in (Daniluk, et al. 2017). You just need to create a key-value attention mechanism where the \"key\" part of the mechanism is used for attention score calculation, and the \"value\" part of the mechanism is used to encode information to create a context vector.  \n",
    "* fill code for one_step_attention function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "avnlc6p9BZDv"
   },
   "outputs": [],
   "source": [
    "def one_step_attention(h: torch.Tensor, s_prev: torch.Tensor, linear_1: nn.Linear, linear_2: nn.Linear) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform one step of attention mechanism which\n",
    "    splits the hidden states of the encoder into key and value,\n",
    "    and uses key and previous hidden state of the decoder to compute attention weights.\n",
    "\n",
    "    :param h: hidden states of the encoder, shape [batch_size, seq_length, hidden_size]\n",
    "    :param s_prev: previous hidden state of the decoder, shape [batch_size, hidden_size]\n",
    "    :param linear_1: nn.Linear for the first linear layer, input size is 2 * hidden_size, output size is hidden_size\n",
    "    :param linear_2: nn.Linear for the second linear layer, input size is hidden_size, output size is 1\n",
    "    :return: context, attention weights\n",
    "    \"\"\"\n",
    "    #Split into Key-Value\n",
    "    _, seq_length, hidden_size = h.size()\n",
    "    half_hidden_size = hidden_size // 2\n",
    "\n",
    "    key, value = torch.split(h, half_hidden_size, dim=2)\n",
    "\n",
    "    s_prev = s_prev.unsqueeze(1).repeat(1, seq_length, 1)\n",
    "    concatenated_input = torch.cat([key, s_prev], dim=2)\n",
    "\n",
    "    #Attention function\n",
    "    M_t = torch.tanh(linear_1(concatenated_input))\n",
    "    alpha_t = F.softmax(linear_2(M_t), dim=1)\n",
    "\n",
    "    context = torch.sum(alpha_t * value, dim=1)\n",
    "\n",
    "    return context, alpha_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zWN02ZtuOIU"
   },
   "source": [
    "# Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0phyUQYg1ZS8"
   },
   "source": [
    "## TODO3: Create and train your encoder/decoder model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "Ji_rUPhK1ZS9"
   },
   "outputs": [],
   "source": [
    "class AttentionModel(L.LightningModule):\n",
    "    def __init__(self, input_vocab: list[str], learning_rate: float = 0.001) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_h = 32 #hidden dimensions for encoder\n",
    "        self.n_s = 32 #hidden dimensions for decoder\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.input_vocab = input_vocab\n",
    "\n",
    "        #encoder can be any RNN of your choice\n",
    "        self.encoder_lstm = nn.LSTM(len(self.input_vocab), self.n_h, batch_first=True)\n",
    "\n",
    "        #decoder has to be (any) RNNCell since we will need to calculate attention for each timestep manually\n",
    "        self.decoder_lstm_cell = nn.LSTMCell(self.n_s//2, self.n_s)\n",
    "        self.output_layer = nn.Linear(self.n_s, len(output_vocab))\n",
    "        \n",
    "        #attention\n",
    "        self.linear_1 = nn.Linear(3 * self.n_s//2, self.n_s)\n",
    "        self.linear_2 = nn.Linear(self.n_s, 1)\n",
    "\n",
    "    def forward(self, src, return_attention=False): #use return_attention only when you want to get the attention scores for visualizing\n",
    "        #pass the input to the encoder\n",
    "        encoder_out, _ = self.encoder_lstm(src)\n",
    "\n",
    "        #Initialize the LSTM states. We have to do this since we are using LSTMCell (https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)\n",
    "        #These states will get updated while we are decoding\n",
    "        decoder_s = torch.randn(src.shape[0], self.n_s).to(self.decoder_lstm_cell.weight_ih.device)\n",
    "        decoder_c = torch.randn(src.shape[0], self.n_s).to(self.decoder_lstm_cell.weight_ih.device)\n",
    "\n",
    "        #Iterate until max_output_length (Decoding)\n",
    "        prediction = torch.zeros((src.shape[0], maxlen_out+1, len(output_vocab))).to(self.decoder_lstm_cell.weight_ih.device)\n",
    "        attention_scores = [] #to store the score for each step\n",
    "        for t in range(maxlen_out):\n",
    "            #Perform one step of the attention mechanism to calculate the context vector at timestep t\n",
    "            context, attention_score = one_step_attention(encoder_out, decoder_s, self.linear_1, self.linear_2)\n",
    "            # Feed the context vector to the decoder.\n",
    "            decoder_s, decoder_c = self.decoder_lstm_cell(context, (decoder_s, decoder_c))\n",
    "            # Pass the decoder hidden output to the output layer (softmax)\n",
    "            out = self.output_layer(decoder_s)\n",
    "            # Put the predicted output into the list for this timestep\n",
    "            prediction[:, t] = out\n",
    "            # Store the attention scores for visualization\n",
    "            attention_scores.append(attention_score)\n",
    "\n",
    "        return (prediction, attention_scores if return_attention else None)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, target = batch\n",
    "        prediction,_ = self(src)\n",
    "        prediction = prediction.reshape(-1, len(output_vocab))\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        src = batch\n",
    "        with torch.no_grad():\n",
    "          prediction, attention_scores = self(src, return_attention=True)\n",
    "          prediction = F.softmax(prediction, dim=-1)\n",
    "          prediction = torch.argmax(prediction, dim=-1)\n",
    "          for pred in prediction:\n",
    "            print(\"\".join([output_idx_to_char[idx.item()] for idx in pred if idx != 0]))\n",
    "        return prediction, attention_scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "pSM9dgDcCz1E"
   },
   "outputs": [],
   "source": [
    "model = AttentionModel(input_vocab, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "RqrvmJalDLzF"
   },
   "outputs": [],
   "source": [
    "data_module = NameDataModule(X, y, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "_sFjzKX8SECo"
   },
   "outputs": [],
   "source": [
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"hw3.1_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "OGWSzS-X1ZTO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=100, logger=wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "7ZMi782c1ZTQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory ./hw3.1_attention/1e51n46e/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name              | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | criterion         | CrossEntropyLoss | 0      | train\n",
      "1 | encoder_lstm      | LSTM             | 12.7 K | train\n",
      "2 | decoder_lstm_cell | LSTMCell         | 6.4 K  | train\n",
      "3 | output_layer      | Linear           | 792    | train\n",
      "4 | linear_1          | Linear           | 1.6 K  | train\n",
      "5 | linear_2          | Linear           | 33     | train\n",
      "---------------------------------------------------------------\n",
      "21.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "21.5 K    Total params\n",
      "0.086     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  26%|██▌       | 89/341 [00:04<00:11, 21.72it/s, v_num=n46e] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:171\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1302\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:123\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/optim/adam.py:202\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 202\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:109\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:241\u001b[0m, in \u001b[0;36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackward_fn\u001b[39m(loss: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:213\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:73\u001b[0m, in \u001b[0;36mPrecision.backward\u001b[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1097\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1097\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5BLw1Ir1ZTT"
   },
   "source": [
    "# Test Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRLjZzBMtCdA"
   },
   "source": [
    "## TODO4: Test your model on 5 examples of your choice including your name!\n",
    "\n",
    "Example Output:\n",
    "```\n",
    "prayutthatha</s></s>aa</s></s>a</s>\n",
    "somchai</s></s></s></s>a</s></s>a</s></s></s></s></s>\n",
    "thanathon</s></s></s></s></s></s></s></s></s></s></s>\n",
    "newin</s>i</s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
    "suthep</s>he</s></s></s></s></s></s></s></s></s></s></s>\n",
    "prawit</s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
    "chatchachatti</s></s>i</s></s></s></s>\n",
    "```\n",
    "\n",
    "<font color='blue'>Paste your model predictions in MyCourseVille</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "6stNACsUP9h-"
   },
   "outputs": [],
   "source": [
    "EXAMPLES = [\"ประยุทธ\", \"สมชาย\", \"ธนาธร\", \"เนวิน\", \"สุเทพ\", \"ประวิตร์\", \"ชัชชาติ\"]\n",
    "\n",
    "# Create test X data\n",
    "test_X = []\n",
    "for name in EXAMPLES:\n",
    "    test_X.append(\n",
    "        [input_char_to_idx[char] for char in name]\n",
    "        + [input_char_to_idx[\"<PAD>\"]] * (maxlen - len(name))\n",
    "    )\n",
    "test_X = torch.tensor(test_X)\n",
    "\n",
    "# Create new collate_fn for test dataloader\n",
    "def collate_fn(batch):\n",
    "    X = batch\n",
    "    X = F.one_hot(torch.stack(X), num_classes=len(input_char_to_idx)).float()\n",
    "    return X\n",
    "\n",
    "# Create test dataloader\n",
    "predict_loader = DataLoader(\n",
    "    test_X, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "kbolC8XIhR3t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionModel(\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (encoder_lstm): LSTM(65, 32, batch_first=True)\n",
       "  (decoder_lstm_cell): LSTMCell(16, 32)\n",
       "  (output_layer): Linear(in_features=32, out_features=24, bias=True)\n",
       "  (linear_1): Linear(in_features=48, out_features=32, bias=True)\n",
       "  (linear_2): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "LsN71S9uQ9wo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]aaaaaaa</s>\n",
      "Predicting DataLoader 0:  14%|█▍        | 1/7 [00:00<00:00, 33.17it/s]aaaaaaa</s>\n",
      "Predicting DataLoader 0:  29%|██▊       | 2/7 [00:00<00:00, 37.54it/s]aaaaaaa</s></s>\n",
      "Predicting DataLoader 0:  43%|████▎     | 3/7 [00:00<00:00, 38.78it/s]aaaaa</s></s>\n",
      "Predicting DataLoader 0:  57%|█████▋    | 4/7 [00:00<00:00, 40.30it/s]aaaaaa</s>\n",
      "Predicting DataLoader 0:  71%|███████▏  | 5/7 [00:00<00:00, 41.44it/s]iaaaaa</s>\n",
      "Predicting DataLoader 0:  86%|████████▌ | 6/7 [00:00<00:00, 41.39it/s]aaaaaaaaa</s></s>\n",
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 41.15it/s]\n"
     ]
    }
   ],
   "source": [
    "output = trainer.predict(model, predict_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7o3893RL1ZT8"
   },
   "source": [
    "## TODO 5: Show your visualization of attention scores on one of your example\n",
    "\n",
    "<font color='blue'>Paste your visualization image in MyCourseVille</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "WHysSqYJ1ZUA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pip install -q seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "XdktVnMv1ZTh"
   },
   "outputs": [],
   "source": [
    "prediction, attention_scores = zip(*output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores shape: (19, 20)\n",
      "Prediction shape: (20,)\n",
      "Attention matrix shape: (8, 20)\n",
      "output_text shape: 20\n",
      "xlabels shape: 8\n"
     ]
    }
   ],
   "source": [
    "show_index = -1\n",
    "\n",
    "# Convert attention scores to numpy and squeeze\n",
    "attention_scores = attention_scores[show_index]\n",
    "attention_scores = torch.cat(attention_scores).squeeze().cpu().numpy()\n",
    "print(\"Attention scores shape:\", attention_scores.shape)\n",
    "\n",
    "# Plot the attention scores\n",
    "prediction = prediction[show_index].squeeze().cpu().numpy()\n",
    "print(\"Prediction shape:\", prediction.shape)\n",
    "\n",
    "# Create a heatmap matrix for the attention scores\n",
    "attn_viz = torch.stack([torch.tensor(attention_scores)]).squeeze().cpu().numpy()\n",
    "attn_viz = attn_viz[: len(EXAMPLES[show_index])+1, : len(prediction)+1]\n",
    "print(\"Attention matrix shape:\", attn_viz.shape)\n",
    "\n",
    "# Create a text and label\n",
    "output_text = [output_idx_to_char[c] for c in prediction]\n",
    "xlabels = [char for char in EXAMPLES[show_index]] + [\"<PAD>\"]\n",
    "print('output_text shape:', len(output_text))\n",
    "print('xlabels shape:', len(xlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "BF6HD99lYlgQ",
    "outputId": "caaa0716-5b99-43e8-950f-dd0127d0fbb7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.heatmap(attn_viz, linewidth=0.5)\n",
    "ax.set_xticklabels(output_text, rotation=60)\n",
    "ax.set_yticklabels(xlabels, rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1UkIsCztaMS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
