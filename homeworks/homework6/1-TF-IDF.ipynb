{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ8FRFIYMc5X"
      },
      "source": [
        "# HOMEWORK 6: TEXT CLASSIFICATION\n",
        "In this homework, you will create models to classify texts from TRUE call-center. There are two classification tasks:\n",
        "1. Action Classification: Identify which action the customer would like to take (e.g. enquire, report, cancle)\n",
        "2. Object Classification: Identify which object the customer is referring to (e.g. payment, truemoney, internet, roaming)\n",
        "\n",
        "We will focus only on the Object Classification task for this homework.\n",
        "\n",
        "In this homework, you are asked compare different text classification models in terms of accuracy and inference time.\n",
        "\n",
        "You will need to build 3 different models.\n",
        "\n",
        "1. A model based on tf-idf\n",
        "2. A model based on MUSE\n",
        "3. A model based on wangchanBERTa\n",
        "\n",
        "**You will be ask to submit 3 different files (.pdf from .ipynb) that does the 3 different models. Finally, answer the accuracy and runtime numbers in MCV.**\n",
        "\n",
        "This homework is quite free form, and your answer may vary. We hope that the processing during the course of this assignment will make you think more about the design choices in text classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHqkFSyaNvOt",
        "outputId": "879b17f1-0fb2-455c-ca37-b5a4aecd7b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-15 13:57:38--  https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 2620:100:6035:18::a27d:5512, 162.125.85.18\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|2620:100:6035:18::a27d:5512|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6 [following]\n",
            "--2025-02-15 13:57:38--  https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6\n",
            "Reusing existing connection to [www.dropbox.com]:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7df6d5a23f54bd3592a4a478d5.dl.dropboxusercontent.com/cd/0/inline/CkIpCV84EQav89R1pHJhwTj_FwQ5tzBwdvR7FObNWcUFXkd7sl7Fq2KLZyPFIH7Hs4r0pmxn_6d6wmuvp63XFYjnZZHu6H_JmSA-R88ygl493TK8-nR_bergdr8EW8mEphs/file# [following]\n",
            "--2025-02-15 13:57:39--  https://uc7df6d5a23f54bd3592a4a478d5.dl.dropboxusercontent.com/cd/0/inline/CkIpCV84EQav89R1pHJhwTj_FwQ5tzBwdvR7FObNWcUFXkd7sl7Fq2KLZyPFIH7Hs4r0pmxn_6d6wmuvp63XFYjnZZHu6H_JmSA-R88ygl493TK8-nR_bergdr8EW8mEphs/file\n",
            "Resolving uc7df6d5a23f54bd3592a4a478d5.dl.dropboxusercontent.com (uc7df6d5a23f54bd3592a4a478d5.dl.dropboxusercontent.com)... 2620:100:6035:15::a27d:550f, 162.125.85.15\n",
            "Connecting to uc7df6d5a23f54bd3592a4a478d5.dl.dropboxusercontent.com (uc7df6d5a23f54bd3592a4a478d5.dl.dropboxusercontent.com)|2620:100:6035:15::a27d:550f|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2518977 (2.4M) [text/plain]\n",
            "Saving to: ‘./clean-phone-data-for-students.csv’\n",
            "\n",
            "./clean-phone-data- 100%[===================>]   2.40M  4.04MB/s    in 0.6s    \n",
            "\n",
            "2025-02-15 13:57:41 (4.04 MB/s) - ‘./clean-phone-data-for-students.csv’ saved [2518977/2518977]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv -O ./clean-phone-data-for-students.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRlx5Mb5zkXw",
        "outputId": "18d913e0-aa6d-435b-931d-591386cb4ba8"
      },
      "outputs": [],
      "source": [
        "!pip install -q pythainlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YprqbOPMc5a"
      },
      "source": [
        "## Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "heICP79cMc5e"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import pandas\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from IPython.display import display\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPaUf4PLMc5k"
      },
      "source": [
        "## Loading data\n",
        "First, we load the data from disk into a Dataframe.\n",
        "\n",
        "A Dataframe is essentially a table, or 2D-array/Matrix with a name for each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JhZ2eBAWMc5l"
      },
      "outputs": [],
      "source": [
        "data_df = pd.read_csv('clean-phone-data-for-students.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cje3yruTMc5p"
      },
      "source": [
        "Let's preview the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "aNqRNz1PMc5q",
        "outputId": "e129a502-1420-476c-dc50-46c293a01b56"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence Utterance</th>\n",
              "      <th>Action</th>\n",
              "      <th>Object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counte...</td>\n",
              "      <td>enquire</td>\n",
              "      <td>payment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n",
              "      <td>enquire</td>\n",
              "      <td>package</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...</td>\n",
              "      <td>report</td>\n",
              "      <td>suspend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...</td>\n",
              "      <td>enquire</td>\n",
              "      <td>internet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...</td>\n",
              "      <td>report</td>\n",
              "      <td>phone_issues</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  Sentence Utterance   Action        Object\n",
              "0   <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counte...  enquire       payment\n",
              "1               internet ยังความเร็วอยุ่เท่าไหร ครับ  enquire       package\n",
              "2   ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...   report       suspend\n",
              "3   พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...  enquire      internet\n",
              "4   ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...   report  phone_issues"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence Utterance</th>\n",
              "      <th>Action</th>\n",
              "      <th>Object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>16175</td>\n",
              "      <td>16175</td>\n",
              "      <td>16175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>13389</td>\n",
              "      <td>10</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>บริการอื่นๆ</td>\n",
              "      <td>enquire</td>\n",
              "      <td>service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>97</td>\n",
              "      <td>10377</td>\n",
              "      <td>2525</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Sentence Utterance   Action   Object\n",
              "count               16175    16175    16175\n",
              "unique              13389       10       33\n",
              "top           บริการอื่นๆ  enquire  service\n",
              "freq                   97    10377     2525"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the top 5 rows\n",
        "display(data_df.head())\n",
        "# Summarize the data\n",
        "data_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGd8BNvMMc5y"
      },
      "source": [
        "## Data cleaning\n",
        "\n",
        "We call the DataFrame.describe() again.\n",
        "Notice that there are 33 unique labels/classes for object and 10 unique labels for action that the model will try to predict.\n",
        "But there are unwanted duplications e.g. Idd,idd,lotalty_card,Lotalty_card\n",
        "\n",
        "Also note that, there are 13389 unqiue sentence utterances from 16175 utterances. You have to clean that too!\n",
        "\n",
        "## #TODO 0.1:\n",
        "You will have to remove unwanted label duplications as well as duplications in text inputs.\n",
        "Also, you will have to trim out unwanted whitespaces from the text inputs.\n",
        "This shouldn't be too hard, as you have already seen it in the demo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "V0bGLblVMc5z",
        "outputId": "1a65aff5-6196-4674-fb5d-36aa1afcfdba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence Utterance</th>\n",
              "      <th>Action</th>\n",
              "      <th>Object</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>16175</td>\n",
              "      <td>16175</td>\n",
              "      <td>16175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>13389</td>\n",
              "      <td>10</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>บริการอื่นๆ</td>\n",
              "      <td>enquire</td>\n",
              "      <td>service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>97</td>\n",
              "      <td>10377</td>\n",
              "      <td>2525</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Sentence Utterance   Action   Object\n",
              "count               16175    16175    16175\n",
              "unique              13389       10       33\n",
              "top           บริการอื่นๆ  enquire  service\n",
              "freq                   97    10377     2525"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n",
              "       'service', 'nonTrueMove', 'balance', 'detail', 'bill', 'credit',\n",
              "       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n",
              "       'information', 'lost_stolen', 'balance_minutes', 'idd',\n",
              "       'TrueMoney', 'garbage', 'Payment', 'IDD', 'ringtone', 'Idd',\n",
              "       'rate', 'loyalty_card', 'contact', 'officer', 'Balance', 'Service',\n",
              "       'Loyalty_card'], dtype=object)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array(['enquire', 'report', 'cancel', 'Enquire', 'buy', 'activate',\n",
              "       'request', 'Report', 'garbage', 'change'], dtype=object)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(data_df.describe())\n",
        "display(data_df.Object.unique())\n",
        "display(data_df.Action.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "19onNNUZMc54"
      },
      "outputs": [],
      "source": [
        "clean_data_time = time.time()\n",
        "\n",
        "# Group the duplicate label\n",
        "data_df.dropna(subset=['Object'], inplace=True)\n",
        "data_df['Object'] = data_df['Object'].apply(lambda x: x.lower())\n",
        "\n",
        "# Clean the data\n",
        "data_df['Sentence Utterance'] = data_df['Sentence Utterance'].apply(lambda x: str(x).strip())\n",
        "data_df['Sentence Utterance'] = data_df['Sentence Utterance'].apply(lambda x: x.lower())\n",
        "data_df.drop_duplicates(subset=['Sentence Utterance'], inplace=True)\n",
        "\n",
        "# Drop the unused columns\n",
        "data_df.drop(columns=['Action'], inplace=True)\n",
        "\n",
        "clean_data_time = time.time() - clean_data_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIxnPRiAmrhN"
      },
      "source": [
        "Split data into train, valdation, and test sets (normally the ratio will be 80:10:10 , respectively). We recommend to use train_test_spilt from scikit-learn to split the data into train, validation, test set.\n",
        "\n",
        "In addition, it should split the data that distribution of the labels in train, validation, test set are similar. There is **stratify** option to handle this issue.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "Make sure the same data splitting is used for all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EYzMrvb7nYR2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 10689\n",
            "Test size: 1336\n",
            "Val size: 1337\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "split_data_time = time.time()\n",
        "\n",
        "# For the object column, we will only keep the object that has more than 2% of the total data\n",
        "object_counter = Counter(data_df['Object'])\n",
        "stratify_col = data_df['Object'].apply(lambda x: 'other' if object_counter[x] < 0.02*len(data_df) else x)\n",
        "train_df, test_df = train_test_split(data_df, test_size=0.2, random_state=4242, stratify=stratify_col)\n",
        "\n",
        "object_counter = Counter(test_df['Object'])\n",
        "stratify_col = test_df['Object'].apply(lambda x: 'other' if object_counter[x] < 0.02*len(test_df) else x)\n",
        "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=4242, stratify=stratify_col)\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "\n",
        "print(f\"Train size: {len(train_df)}\")\n",
        "print(f\"Test size: {len(test_df)}\")\n",
        "print(f\"Val size: {len(val_df)}\")\n",
        "\n",
        "split_data_time = time.time() - split_data_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the data\n",
        "# train_df.to_csv('train.csv', index=False)\n",
        "# test_df.to_csv('test.csv', index=False)\n",
        "# val_df.to_csv('val.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx6gllzrnVVU"
      },
      "source": [
        "# Model 1 TF-IDF\n",
        "\n",
        "Build a model to train a tf-idf text classifier. Use a simple logistic regression model for the classifier.\n",
        "\n",
        "For this part, you may find this [tutorial](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py) helpful.\n",
        "\n",
        "Below are some design choices you need to consider to accomplish this task. Be sure to answer them when you submit your model.\n",
        "\n",
        "What tokenizer will you use? Why?\n",
        "\n",
        "**Ans:** Use `newmm` tokenizer from PyThaiNLP. It is a dictionary-based tokenizer that is suitable for Thai text. Additionally, I use `ngram_range=(1, 2)` to include bigram in the feature set. This is because bigram can capture the relationship between words that are close to each other.\n",
        "\n",
        "Will you ignore some stop words (a, an, the, to, etc. for English) in your tf-idf? Is it important?\n",
        "PythaiNLP provides a list of stopwords if you want to use (https://pythainlp.org/docs/2.0/api/corpus.html#pythainlp.corpus.common.thai_stopwords)\n",
        "\n",
        "**Ans:** I use the list of stopwords provided by PyThaiNLP. It is important to remove stopwords because they are common words that do not carry much meaning. Removing them can help the model focus on more important words.\n",
        "\n",
        "The dictionary of TF-IDF is usually based on the training data. How many words in the test set are OOVs?\n",
        "\n",
        "**Ans:** 324 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9vOqTqmfufsT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "train_df, test_df, val_df = pd.read_csv('train.csv'), pd.read_csv('test.csv'), pd.read_csv('val.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['กระไร', 'กาลนาน', 'ชิ้น', 'ดังที่', 'ดี', 'ดีกว่า', 'ด้อย', 'ตัว', 'ต่อไป', 'ถัดไป', 'ทั่วถึง', 'ทำ', 'ที่จะ', 'ท่าน', 'ท้าย', 'นา', 'บอ', 'บัด', 'ระยะเวลา', 'ล่ะ', 'วันวาน', 'สม', 'สมบูรณ์', 'สํา', 'หน้า', 'หรับ', 'หา', 'อย', 'เกี่ยว', 'เก่า', 'เดี๋ยวนี้', 'เย็น', 'เล่า', 'เสมือน', 'เหมือนกัน', 'แด่', 'แม้น', 'แหล่', 'โง้น', 'โน้น', 'ใด', 'ไว', 'ไหม', '\\ufeff'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenize_and_vectorize_time = time.time()\n",
        "\n",
        "# Create the tf-idf vectorizer\n",
        "tf_idf = TfidfVectorizer(tokenizer=pythainlp.word_tokenize, \n",
        "                         ngram_range=(1, 2),\n",
        "                         stop_words=list(pythainlp.corpus.thai_stopwords()))\n",
        "tf_idf.fit(train_df['Sentence Utterance'])\n",
        "\n",
        "# Transform the data\n",
        "train_x = tf_idf.transform(train_df['Sentence Utterance'])\n",
        "test_x = tf_idf.transform(test_df['Sentence Utterance'])\n",
        "val_x = tf_idf.transform(val_df['Sentence Utterance'])\n",
        "\n",
        "tokenize_and_vectorize_time = time.time() - tokenize_and_vectorize_time\n",
        "\n",
        "# Get y label\n",
        "train_y = train_df['Object']\n",
        "test_y = test_df['Object']\n",
        "val_y = val_df['Object']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_time = time.time()\n",
        "\n",
        "# Create the model\n",
        "model = LogisticRegression(max_iter=1000, penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0, n_jobs=-1)\n",
        "model.fit(train_x, train_y)\n",
        "\n",
        "train_time = time.time() - train_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train accuracy: 0.7176536626438395\n",
            "Test accuracy: 0.6803892215568862\n",
            "Val accuracy: 0.6798803290949887\n"
          ]
        }
      ],
      "source": [
        "all_inference_time = time.time()\n",
        "\n",
        "# Predict the data\n",
        "train_pred = model.predict(train_x)\n",
        "test_pred = model.predict(test_x)\n",
        "val_pred = model.predict(val_x)\n",
        "\n",
        "all_inference_time = time.time() - all_inference_time\n",
        "\n",
        "# Calculate the accuracy\n",
        "train_acc = accuracy_score(train_y, train_pred)\n",
        "test_acc = accuracy_score(test_y, test_pred)\n",
        "val_acc = accuracy_score(val_y, val_pred)\n",
        "\n",
        "print(f\"Train accuracy: {train_acc}\")\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "print(f\"Val accuracy: {val_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val classification report\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "        balance       0.75      0.78      0.76       148\n",
            "balance_minutes       0.80      0.80      0.80         5\n",
            "           bill       0.65      0.48      0.55        54\n",
            "         credit       1.00      0.76      0.87        17\n",
            "         detail       0.89      0.24      0.38        33\n",
            "        garbage       0.00      0.00      0.00         6\n",
            "            idd       0.82      0.64      0.72        14\n",
            "    information       1.00      0.31      0.47        29\n",
            "       internet       0.66      0.82      0.73       179\n",
            "       iservice       0.00      0.00      0.00         3\n",
            "    lost_stolen       0.93      0.90      0.92        30\n",
            "   loyalty_card       1.00      0.62      0.77         8\n",
            " mobile_setting       0.85      0.39      0.54        28\n",
            "    nontruemove       0.89      0.38      0.53        21\n",
            "        officer       0.00      0.00      0.00         1\n",
            "        package       0.62      0.72      0.67       179\n",
            "        payment       0.58      0.66      0.61        64\n",
            "   phone_issues       0.63      0.57      0.60        58\n",
            "      promotion       0.56      0.55      0.56       115\n",
            "           rate       0.00      0.00      0.00         6\n",
            "       ringtone       1.00      0.91      0.95        11\n",
            "        roaming       0.85      0.65      0.73        17\n",
            "        service       0.66      0.82      0.73       211\n",
            "        suspend       0.70      0.67      0.69        73\n",
            "      truemoney       0.83      0.74      0.78        27\n",
            "\n",
            "       accuracy                           0.68      1337\n",
            "      macro avg       0.67      0.54      0.57      1337\n",
            "   weighted avg       0.69      0.68      0.67      1337\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Users/jirayuwat/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Print the classification report\n",
        "print(\"Val classification report\")\n",
        "print(classification_report(val_y, val_pred))\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save the classificaion report\n",
        "classification_report_dict = classification_report(val_y, val_pred, output_dict=True)\n",
        "with open('classification_report_tfidf.pkl', 'wb') as f:\n",
        "    pickle.dump(classification_report_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All preprocessing time: 1.20 seconds\n",
            " - Clean data time: 0.02 seconds\n",
            " - Split data time: 0.03 seconds\n",
            " - Tokenize and vectorize time: 1.15 seconds\n",
            "Train time: 34.22 seconds\n",
            "Inference time: 0.01 seconds\n"
          ]
        }
      ],
      "source": [
        "print(f'''\n",
        "All preprocessing time: {clean_data_time + split_data_time + tokenize_and_vectorize_time:.2f} seconds\n",
        " - Clean data time: {clean_data_time:.2f} seconds\n",
        " - Split data time: {split_data_time:.2f} seconds\n",
        " - Tokenize and vectorize time: {tokenize_and_vectorize_time:.2f} seconds\n",
        "Train time: {train_time:.2f} seconds\n",
        "Inference time: {all_inference_time:.2f} seconds\n",
        "'''.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OOV words: 324\n",
            "OOV words: {'เหล่านี้', 'ไทร', 'เหตุการณ์', 'เชียงคาน', 'สเวิด', 'ประวัติการ', 'การผ่อน', 'lift', 'คล่องตัว', '4541843', 'ละคร', 'เลค', 'siii', 'ช่า', 'งก', '3000', 'โ', 'session', 'ร้อง', 'มาม่า', 'แฟ๊กซ์', 'ปราจีน', 'เพ็จ', 'บัตรประจำตัวประชาชน', 'ปิ่น', 'ไบท์', 'พี๋', 'บถ', 'เปิดบัญชี', 'word', 'เทอร์เนต', 'มีเดีย', 'ipod', 'เนีตดทร', 'ีน', 'นาฬิกา', 'clubbinbit', 'อรืค', '081', 'รัง', 'i-mobile', 'วันหนึ่ง', 'แพ้', 'คร่ะ', 'วดี', 'นึง่', 'เล็จ', 'เหตุผล', 'แกแล', 'small', 'รักษา', 'ซ่า', '8520', 'ยกตัว', 'ผิดนัด', 'พาราไดซ์', 'เกล้า', 'ทำไปทำมา', 'เบาะ', 'ตามจริง', 'ตกน้ำ', 'เกด', 'ลา่', 'ยุทธ', 'ดัน', 'เมลล์', 'ลักษณะ', '3276.25', 'สิงค์', 'บู', 'javagame', 'สัมปทาน', 'ซิ่', 'ขึ่น', 'ii', 'วงกลม', 'ขั้น', '3057.79', 'เฟิร์ส', '(*777*913#', 'เมธา', 'แป็น', 'เยอรมัน', 'ระบบเครือข่าย', 'ริก', 'ษา', 'พุก', 'surf', 'สิงหา', 'คิดราคา', 'ลาย', 'ตัวเรา', 'เสื่อม', 'เอ๊ะ', 'กล่องดำ', 'เดิน', '\"insearch', 'น้ำ', 'สุรา', 'sii', 'ความขัดข้อง', 'ปิยะ', 'เฟ๊ค', 'เซฟ', 'ริ้น', 'ข้างหลัง', 'เชิญชวน', 'กระจาย', 'active', '(vms)', 'เลบ', 'ลูกพี่', 'กันยายน', 'เครียร์', 'iplay', 'สระแก้ว', 'รดี', 'แอปเปิ้ล', 'เนตไมด', 'เบอร์รี', 'offline', 'ผลิตภัณฑ์', 'แป้นพิมพ์', 'ปรับตัว', 'คเก็ต', 'มีทาง', 'โอ้', 'แช็ต', 'สมัต', 'ด้วยตัวเอง', 'ฮัทห', 'เวิร์ท', 'ทติ้ง', 'นต', 'เชื่อ', 'แท็ปเลด', 'แปลง', 'โครงข่าย', 'กล', 'music', 'เมื่อครู่นี้', 'คูณ', 'ขายขาด', 'ต่ำสุด', 'ทางด่วน', '349', 'เงียบกริบ', 'แทป', 'กิตติ', 'หมอดู', '365', 'ธิติ', 'ทุกๆ', 'กรุ่น', 'รูปแบบ', 'ใต้', 'ข้างใน', '1922', 'แพ๊คเก็ต', 'ทวี', 'ทาวเวอร์', 'ฟบิ้ง', 'grand', 'date', 'หสย', 'แล้วจึง', '1977', 'สต๊อป', 'e-banking', 'ห้าม', 'point', 'วรรณ', '1,023', 'เม๊ก', 'กัมพูชา', 'ลองเครื่อง', 'pass', 'แวะ', 'คร้าบ', 'สไตล์', 'เป', 'พฤหัส', 'อวแอ๊ป', 'ดิจิตอล', 'จธ', 'เซ็นเตอร์', 'ธิดา', 'เน๊ตจ', 'จห', 'กลุ่มเป้าหมาย', 'ซือ', '750', 'รหัสทางไกล', 'ตอนกลางวัน', 'day', 'ตังส์', '”', 'ซ้อ', 'เช็ต', 'สุชา', 'สัง', '<phone_number_removed>2284', 'เวอท', '31', '4846', 'เพ็ค', 'เน๊ตให่', 'รับเงิน', '“', 'ตรีทศ', 'ธัญ', 'สายไหม', 'สองเท่า', 'รุ้ง', 'นี้แหละ', '759', 'url', 'เท่านั้นเอง', '4877000', 'cms', 'เจ้านาย', 'ลลูล่าร์', 'ปัท', 'คุณแม่', '412', '5460', '332', 'masseage', '้', 'hi-speed', '087', 'สัีญญา', 'วื่ง', 'daily', 'ชั่ง', 'เป็นเรื่อง', '*777*807', 'หลวงพี่', 'จั๊ม', 'ต่อตัว', 'สิ้นปี', 'ดุ', 'เป๋า', 'ย้าง', 'วลิ่ง', 'เอาล์', 'แบ็กเบอ', 'and', 'แล้วไปแล้ว', 'จอ', 'ภาษาจีน', 'counter', 'เปืดส', 'ยาด', 'สห', 'เบอ์', 'เพือ', 'นอกจาก', 'ปัญญา', 'asdl', 'ออร์', '-xxxxxxx', 'แผนก', 'ป่า', 'สดี', 'ต่อไม่ติด', 'เกตุ', 'เว้', 'คืว่า', 'เครื่องหมายคำถาม', 'ต้าง', 'บืล', 'นอกเหนือจาก', 'ดีแล้ว', 'spicy', 'รือ', 'เคลื่อน', 'น้', 'uim', 'ชือ', 'แกะ', 'โกไล', 'ผ่านหน้า', 'นน', 'ถึงกำหนด', 'new', 'พระประแดง', 'ส่งสัญญาณ', 'ความตรง', 'ล้วง', 'atb', '408', 'ร๊อค', 'นราธิวาส', 'อายัต', 'ไซ', 'พอสมควร', 'หด', 'due', 'ktb', 'เป็นปกติ', 'ษ', 'กูเกิล', 'ไม่เป็นไร', '*777*915#)', 'เล้ว', 'มืต', 'ตรวน', 'ฮัช', 'laptop', 'cinemax', 'ใส้', 'บู้', 'hbo', 'ซิงค์', '4899889', '??', '\"', 'ทับซ้อน', 'มีสิทธิ์', 'ท๊อปอัพ', 'คร่า', 'นุ', 'คทา'}\n"
          ]
        }
      ],
      "source": [
        "# Count OOV words\n",
        "tokenizer = pythainlp.word_tokenize\n",
        "\n",
        "train_words = set()\n",
        "for sentence in train_df['Sentence Utterance']:\n",
        "    train_words.update(tokenizer(sentence))\n",
        "\n",
        "test_words = set()\n",
        "for sentence in test_df['Sentence Utterance']:\n",
        "    test_words.update(tokenizer(sentence))\n",
        "\n",
        "val_words = set()\n",
        "for sentence in val_df['Sentence Utterance']:\n",
        "    val_words.update(tokenizer(sentence))\n",
        "\n",
        "oov_words = test_words.union(val_words) - train_words\n",
        "print(f\"OOV words: {len(oov_words)}\")\n",
        "print(\"OOV words:\", oov_words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
