{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EdbVSmE87En"
   },
   "source": [
    "# HW1.1: Dictionary-based Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJJLm1Ub87Et"
   },
   "source": [
    "In this exercise, you are to implement a dictionary-based word segmentation algorithm. There are two Python functions that you need to complete:\n",
    "<br>\n",
    "* maximal_matching\n",
    "* backtrack\n",
    "</br>\n",
    "\n",
    "Also, you have to find how to use word_tokenize() in PythaiNLP along with customer_dict by yourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yXxPBOcNLXfm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq pythainlp\n",
    "%pip install -qq marisa_trie\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import get_corpus\n",
    "from marisa_trie import Trie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLGgO8PrLSz6"
   },
   "source": [
    "## Part 1) Maximal Matching from PythaiNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzs0R06q87Et"
   },
   "source": [
    "### Create a toy dictionary to test the algorithm\n",
    "\n",
    "This is based on the example shown in the lecture.\n",
    "You will tokenize the following text string: \"ไปหามเหสี!\"\n",
    "The toy dictionary provided in this exercise includes all the charaters, syllables, and words that appear that the text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pq3W4p3z87Ev"
   },
   "outputs": [],
   "source": [
    "thai_vocab = [\"ไ\", \"ป\", \"ห\", \"า\", \"ม\", \"เ\", \"ห\", \"ส\", \"ี\", \"ไป\", \"หา\", \"หาม\", \"เห\", \"สี\", \"มเหสี\", \"!\"]\n",
    "input_text = \"ไปหามเหสี!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrZrzQoXLeUX"
   },
   "source": [
    "### Example Dictionary\n",
    "\n",
    "Write the `word_tokenize` function of PyThaiNLP with a custom dictionary above and using:\n",
    "1. Longest matching algorithm `longest`\n",
    "2. Maximal-matching algorithm `newmm`\n",
    "\n",
    "Study `word_tokenize()` from PythaiNLP in the link below. Note: `custom_dict` will accept Trie structures as `Trie(iterable)`.\n",
    "\n",
    "https://pythainlp.org/docs/5.0/api/tokenize.html#pythainlp.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IhCPz25GIbJv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest matching\n",
      "\tTokenize: ['ไป', 'หาม', 'เห', 'สี', '!']\n",
      "Maximal matching\n",
      "\tTokenize: ['ไป', 'หา', 'มเหสี', '!']\n"
     ]
    }
   ],
   "source": [
    "####FILL CODE HERE####\n",
    "custom_dict = Trie(thai_vocab)\n",
    "print(\"Longest matching\")\n",
    "print(f'\\tTokenize: {word_tokenize(input_text, custom_dict=custom_dict, engine=\"longest\")}')\n",
    "\n",
    "print(\"Maximal matching\")\n",
    "print(f'\\tTokenize: {word_tokenize(input_text, custom_dict=custom_dict, engine=\"newmm\")}')\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF5Pme7CK3YF"
   },
   "source": [
    "## Part 2) Maximal Matching from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZornooGF87Ew"
   },
   "source": [
    "### Maximal matching\n",
    "Complete the maximal matching function below with dynamic programming to tokenize the input text and output the 2D numerical array shown in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ao4d2E3387Ew"
   },
   "outputs": [],
   "source": [
    "from math import inf  # infinity\n",
    "\n",
    "\n",
    "def maximal_matching(c: str | list[chr]) -> list[list[int]]:\n",
    "    # Create custom_dict\n",
    "    custom_dict = Trie(thai_vocab)\n",
    "\n",
    "    # Initialize an empty 2D list\n",
    "    d = [[None] * len(c) for _ in range(len(c))]\n",
    "\n",
    "    ####FILL CODE HERE####\n",
    "    # Initialize the first row\n",
    "    for column in range(len(c)):\n",
    "        d[0][column] = 1 if c[0 : column + 1] in custom_dict else inf\n",
    "\n",
    "    # Fill in the rest of the table\n",
    "    for row in range(1, len(c)):\n",
    "        # Reset the row\n",
    "        d[row][:row] = [None] * row\n",
    "        for column in range(row, len(c)):\n",
    "            # Check if the current word is in the dictionary\n",
    "            current_word = c[row : column + 1]\n",
    "            if current_word in custom_dict:\n",
    "                # If the current word is in the dictionary, the cost is 1 + the minimum cost of the previous row\n",
    "                d[row][column] = 1 + min([d[k][row - 1] for k in range(0, row)])\n",
    "            else:\n",
    "                # If the current word is not in the dictionary, the cost is infinity\n",
    "                d[row][column] = inf\n",
    "    ######################\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7vBXfjM87Ew"
   },
   "source": [
    "### Backtracking\n",
    "Complete the backtracking function below to find the tokenzied words.\n",
    "It should return a list containing a pair of the beginning position and the ending position of each word.\n",
    "In this example, it should return:\n",
    "<br>\n",
    "[(0, 1),(2, 3),(4, 8),(9, 9)]\n",
    "<br>\n",
    "#### Each pair contains the position of each word as follows:\n",
    "(0, 1) ไป\n",
    "<br>\n",
    "(2, 3) หา\n",
    "<br>\n",
    "(4, 8) มเหสี\n",
    "<br>\n",
    "(9, 9) !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SxNFf1IE87Ex"
   },
   "outputs": [],
   "source": [
    "def backtrack(d: list[list[int]]) -> list[tuple[int, int]]:\n",
    "    eow = len(d) - 1  # End of Word position\n",
    "    word_pos = []  # Word position\n",
    "    ####FILL CODE HERE####\n",
    "\n",
    "    while eow >= 0:\n",
    "        for start in range(eow + 1):\n",
    "            if d[start][eow] != inf and d[start][eow] is not None:\n",
    "                # Found a valid token boundary\n",
    "                word_pos.append((start, eow))\n",
    "                eow = start - 1  # Move to the previous segment\n",
    "                break\n",
    "\n",
    "    ######################\n",
    "    word_pos.reverse()\n",
    "    return word_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0MJkKsh87Ex"
   },
   "source": [
    "### Test your maximal matching algorithm on a toy dictionary\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "[1   ,    1,  inf,  inf,  inf,  inf,  inf,  inf,  inf, inf] ไ\n",
    "[None,    2,  inf,  inf,  inf,  inf,  inf,  inf,  inf, inf] ป\n",
    "[None, None,    2,    2,    2,  inf,  inf,  inf,  inf, inf] ห\n",
    "[None, None, None,    3,  inf,  inf,  inf,  inf,  inf, inf] า\n",
    "[None, None, None, None,    3,  inf,  inf,  inf,    3, inf] ม\n",
    "[None, None, None, None, None,    3,    3,  inf,  inf, inf] เ\n",
    "[None, None, None, None, None, None,    4,  inf,  inf, inf] ห\n",
    "[None, None, None, None, None, None, None,    4,    4, inf] ส\n",
    "[None, None, None, None, None, None, None, None,    5, inf]  ี\n",
    "[None, None, None, None, None, None, None, None, None,   4] !\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tsmVQIKS87Ey"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n",
      "[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n",
      "[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n",
      "[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n",
      "[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n",
      "[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n",
      "[None, None, None, None, None, None, 4, inf, inf, inf] ห\n",
      "[None, None, None, None, None, None, None, 4, 4, inf] ส\n",
      "[None, None, None, None, None, None, None, None, 5, inf] ี\n",
      "[None, None, None, None, None, None, None, None, None, 4] !\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ไปหามเหสี!\"\n",
    "out = maximal_matching(input_text)\n",
    "for i in range(len(out)):\n",
    "    print(out[i], input_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVhCMM4d87Ey"
   },
   "source": [
    "### Test your backtracking algorithm on a toy dictionary\n",
    "Compare your results with the result from PyThaiNLP `newmm`.\n",
    "\n",
    "Expected output:\n",
    "<br>\n",
    "ไป|หา|มเหสี|!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6Hurbm1f87Ey"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ไป|หา|มเหสี|!\n"
     ]
    }
   ],
   "source": [
    "def print_tokenized_text(d, input_text):\n",
    "    tokenized_text = []\n",
    "    for pos in backtrack(d):\n",
    "        # print(pos)\n",
    "        tokenized_text.append(input_text[pos[0] : pos[1] + 1])\n",
    "\n",
    "    print(\"|\".join(tokenized_text))\n",
    "\n",
    "\n",
    "print_tokenized_text(out, input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mZ226USPICM"
   },
   "source": [
    "### <font color=blue>Question 1</font>\n",
    "Using your maximal matching code with the toy dictionary, how many “words” did you get when tokenizing this input text.\n",
    "\n",
    "Answer this question in question #1 in MyCourseVille. Also print out the answer in this notebook as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C4c6gX4ySfUh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: ไปหาหมามเหสีมาหาม!\n",
      "Output positions: [(0, 1), (2, 3), (4, 4), (5, 5), (6, 6), (7, 11), (12, 12), (13, 13), (14, 16), (17, 17)]\n",
      "Token amount: 10\n",
      "ไป|หา|ห|ม|า|มเหสี|ม|า|หาม|!\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ไปหาหมามเหสีมาหาม!\"\n",
    "\n",
    "out = maximal_matching(input_text)\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Output positions: {backtrack(out)}\")\n",
    "print(f\"Token amount: {len(backtrack(out))}\")\n",
    "print_tokenized_text(out, input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57rP9cTU87Ez"
   },
   "source": [
    "## Part 3) Your Maximal Matching with Real Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V306h7AG87Ez"
   },
   "source": [
    "For UNIX-based OS users, the following cell will download a dictionary (it's just a list of thai words). Alternatively, you can download it from this link: https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EFVR9LO187Ez",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-08 13:47:43--  https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1519589 (1.4M) [text/plain]\n",
      "Saving to: ‘words_th.txt.4’\n",
      "\n",
      "words_th.txt.4      100%[===================>]   1.45M  --.-KB/s    in 0.06s   \n",
      "\n",
      "2025-01-08 13:47:44 (24.4 MB/s) - ‘words_th.txt.4’ saved [1519589/1519589]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nqIQzVgE87E0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 62077\n",
      "['ก ข ไม่กระดิกหู', 'ก.', 'ก.ค.', 'ก.ต.', 'ก.ป.ส.', 'ก.พ.', 'ก.พ.ด.', 'ก.ม.', 'ก.ย.', 'ก.ย']\n"
     ]
    }
   ],
   "source": [
    "with open(\"words_th.txt\", encoding=\"utf-8-sig\") as f:\n",
    "    thai_vocab = f.read().splitlines()\n",
    "print(\"Vocab size:\", len(thai_vocab))\n",
    "print(thai_vocab[:10])\n",
    "\n",
    "thai_vocab.extend([\"ๆ\", \"!\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kpjwzw1w87E0"
   },
   "source": [
    "### Part 3.1) The output of **YOUR** maximal matching algoithm on the new dictionary\n",
    "Expected output:\n",
    "```\n",
    "[ inf,    1,  inf,    1,  inf,  inf,  inf,  inf,  inf] ไ\n",
    "[None,  inf,  inf,  inf,  inf,  inf,  inf,  inf,  inf] ป\n",
    "[None, None,  inf,    2,    2,  inf,  inf,  inf,  inf] ห\n",
    "[None, None, None,  inf,  inf,  inf,  inf,  inf,  inf] า\n",
    "[None, None, None, None,  inf,  inf,  inf,  inf,    2] ม\n",
    "[None, None, None, None, None,  inf,    3,  inf,  inf] เ\n",
    "[None, None, None, None, None, None,  inf,  inf,  inf] ห\n",
    "[None, None, None, None, None, None, None,  inf,    4] ส\n",
    "[None, None, None, None, None, None, None, None,  inf] ี\n",
    "```\n",
    "### Expected tokenized text\n",
    "ไปหา|มเหสี\n",
    "\n",
    "_Question: Why are the resulting tokens different?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lYD5ChIS87E0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf, 1, inf, 1, inf, inf, inf, inf, inf] ไ\n",
      "[None, inf, inf, inf, inf, inf, inf, inf, inf] ป\n",
      "[None, None, inf, 2, 2, inf, inf, inf, inf] ห\n",
      "[None, None, None, inf, inf, inf, inf, inf, inf] า\n",
      "[None, None, None, None, inf, inf, inf, inf, 2] ม\n",
      "[None, None, None, None, None, inf, 3, inf, inf] เ\n",
      "[None, None, None, None, None, None, inf, inf, inf] ห\n",
      "[None, None, None, None, None, None, None, inf, 4] ส\n",
      "[None, None, None, None, None, None, None, None, inf] ี\n",
      "ไปหา|มเหสี\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ไปหามเหสี\"\n",
    "out = maximal_matching(input_text)\n",
    "for i in range(len(out)):\n",
    "    print(out[i], input_text[i])\n",
    "\n",
    "print_tokenized_text(out, input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajiIkN-SQKCq"
   },
   "source": [
    "### <font color=blue>Question 2</font>\n",
    "Using your maximal matching algorithm and the actual Thai dictionary, how many “words” did you get when tokenizing this input text.\n",
    "\n",
    "Answer this question in question #2 in MyCourseVille. Also print out the answer in this notebook as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "G5Kd9_z-QV9D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: ประเทศไทยรวมเลือดเนื้อชาติเชื้อไทยเป็นประชารัฐไผทของไทยทุกส่วนอยู่ดำรงคงไว้ได้ทั้งมวลด้วยไทยล้วนหมายรักสามัคคี\n",
      "Output positions: [(0, 5), (6, 8), (9, 11), (12, 21), (22, 25), (26, 30), (31, 33), (34, 37), (38, 42), (43, 45), (46, 48), (49, 51), (52, 54), (55, 57), (58, 61), (62, 65), (66, 69), (70, 74), (75, 77), (78, 84), (85, 88), (89, 91), (92, 95), (96, 99), (100, 102), (103, 109)]\n",
      "Token amount: 26\n",
      "ประเทศ|ไทย|รวม|เลือดเนื้อ|ชาติ|เชื้อ|ไทย|เป็น|ประชา|รัฐ|ไผท|ของ|ไทย|ทุก|ส่วน|อยู่|ดำรง|คงไว้|ได้|ทั้งมวล|ด้วย|ไทย|ล้วน|หมาย|รัก|สามัคคี\n"
     ]
    }
   ],
   "source": [
    "input_text = \"ประเทศไทยรวมเลือดเนื้อชาติเชื้อไทยเป็นประชารัฐไผทของไทยทุกส่วนอยู่ดำรงคงไว้ได้ทั้งมวลด้วยไทยล้วนหมายรักสามัคคี\"\n",
    "\n",
    "out = maximal_matching(input_text)\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Output positions: {backtrack(out)}\")\n",
    "print(f\"Token amount: {len(backtrack(out))}\")\n",
    "print_tokenized_text(out, input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqtT1ISUHY_I"
   },
   "source": [
    "### Part 3.2) Use PyThaiNLP `word_tokenize` with custom dictionary\n",
    "\n",
    "Try tokenizing the following text with `word_tokenize` in `newmm` algorithm and default real dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "goQE5gFUL4KO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['นัด', 'กินกัน', 'ตอน', 'ไหน', 'ก็', 'ได้ที่', 'สามย่าน', 'มิตร', 'ทาวน์']\n"
     ]
    }
   ],
   "source": [
    "text = \"นัดกินกันตอนไหนก็ได้ที่สามย่านมิตรทาวน์\"\n",
    "\n",
    "####FILL CODE HERE####\n",
    "\n",
    "custom_dict = Trie(thai_vocab)\n",
    "token = word_tokenize(text, custom_dict=custom_dict, engine=\"newmm\")\n",
    "print(token)\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SlX5cEBMHPd"
   },
   "source": [
    "Add 'สามย่านมิตรทาวน์' into dictionary and then tokenize again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "b4V9TqFaMPAj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['นัด', 'กินกัน', 'ตอน', 'ไหน', 'ก็', 'ได้ที่', 'สามย่านมิตรทาวน์']\n"
     ]
    }
   ],
   "source": [
    "####FILL CODE HERE####\n",
    "\n",
    "new_thai_vocab = thai_vocab + [\"สามย่านมิตรทาวน์\"]\n",
    "new_custom_dict = Trie(new_thai_vocab)\n",
    "\n",
    "token = word_tokenize(text, custom_dict=new_custom_dict, engine=\"newmm\")\n",
    "print(token)\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1Lww7VVSmYj"
   },
   "source": [
    "### <font color=blue>Question 3</font>\n",
    "Using the code from part three only, how many “words” did you get when tokenizing this input text **after adding the new vocabs**.\n",
    "\n",
    "Answer this question in question #3 in MyCourseVille. Also print out the answer in this notebook as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "E_JHjViLajJi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: อ๋อก็ว่าจะไปเรียนแต่งหน้านั่งสมาธิดำน้ำปลูกปะการังทำอาหารนวดสปาปลูกป่าดำนาดูดิสนีย์ออนไอซ์แรลลี่ตีกอล์ฟล่องเรือส่องสัตว์ช้อปปิ้งดูงิ้วดูละครเวทีดูคอนเสิร์ตดินเนอร์ทำขนมจัดดอกไม้เที่ยวตลาดน้ำเรียนถ่ายรูปดูกายกรรมชมเมืองเก่าเข้าสัมมนาทัวร์ธรรมมะเรียนเต้นแล้วก็ร้องเพลง\n",
      "Output positions: [(0, 2), (3, 4), (5, 7), (8, 9), (10, 11), (12, 16), (17, 24), (25, 33), (34, 38), (39, 42), (43, 49), (50, 56), (57, 59), (60, 62), (63, 69), (70, 73), (74, 75), (76, 89), (90, 95), (96, 102), (103, 110), (111, 119), (120, 127), (128, 129), (130, 133), (134, 135), (136, 143), (144, 145), (146, 154), (155, 162), (163, 164), (165, 167), (168, 170), (171, 176), (177, 182), (183, 189), (190, 194), (195, 201), (202, 203), (204, 210), (211, 212), (213, 217), (218, 221), (222, 225), (226, 231), (232, 236), (237, 242), (243, 247), (248, 251), (252, 257), (258, 265)]\n",
      "Token amount: 51\n"
     ]
    }
   ],
   "source": [
    "new_vocab = [\"ดิสนีย์ออนไอซ์\", \"ตีกอล์ฟ\", \"ธรรมมะ\"]\n",
    "input_text = \"อ๋อก็ว่าจะไปเรียนแต่งหน้านั่งสมาธิดำน้ำปลูกปะการังทำอาหารนวดสปาปลูกป่าดำนาดูดิสนีย์ออนไอซ์แรลลี่ตีกอล์ฟล่องเรือส่องสัตว์ช้อปปิ้งดูงิ้วดูละครเวทีดูคอนเสิร์ตดินเนอร์ทำขนมจัดดอกไม้เที่ยวตลาดน้ำเรียนถ่ายรูปดูกายกรรมชมเมืองเก่าเข้าสัมมนาทัวร์ธรรมมะเรียนเต้นแล้วก็ร้องเพลง\"\n",
    "\n",
    "thai_vocab = thai_vocab + new_vocab\n",
    "out = maximal_matching(input_text)\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Output positions: {backtrack(out)}\")\n",
    "print(f\"Token amount: {len(backtrack(out))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKZEBhpQTeiA"
   },
   "source": [
    "## Part 4) Use maximal matching on real dataset\n",
    "\n",
    "To complete this exercise, we will use the maximal matching algorithm on NECTEC's BEST corpus.\n",
    "\n",
    "The corpus has a structure of characters with target whether it's a beginning of a word (True/False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "mdw7k8CzUSK4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1EcrlXYUyIEM3aeIJse6nPpiv_UjSKgoU&confirm=t\n",
      "To: /Users/jirayuwat/Desktop/2110572-NLP/homework1/corpora.tar.gz\n",
      "100%|████████████████████████████████████████| 121M/121M [00:06<00:00, 18.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "%pip install -q gdown\n",
    "!gdown \"1EcrlXYUyIEM3aeIJse6nPpiv_UjSKgoU&confirm=t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBdyPTEGUXDW"
   },
   "outputs": [],
   "source": [
    "!tar xvf corpora.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ndv-whcXU2_5"
   },
   "outputs": [],
   "source": [
    "%pip install -q pandas\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "l6fgMSRYUY5G"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ป</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ฏ</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ิ</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ร</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ู</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644911</th>\n",
       "      <td>ห</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644912</th>\n",
       "      <td>น</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644913</th>\n",
       "      <td>ม</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644914</th>\n",
       "      <td>า</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644915</th>\n",
       "      <td>\"</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2271932 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       char  target\n",
       "0         ป    True\n",
       "1         ฏ   False\n",
       "2         ิ   False\n",
       "3         ร   False\n",
       "4         ู   False\n",
       "...     ...     ...\n",
       "644911    ห   False\n",
       "644912    น   False\n",
       "644913    ม    True\n",
       "644914    า   False\n",
       "644915    \"    True\n",
       "\n",
       "[2271932 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the preprocessed data\n",
    "best_processed_path = \"corpora/BEST\"\n",
    "option = \"test\"\n",
    "\n",
    "df = []\n",
    "# article types in BEST corpus\n",
    "article_types = [\"article\", \"encyclopedia\", \"news\", \"novel\"]\n",
    "for article_type in article_types:\n",
    "    df.append(pd.read_csv(os.path.join(best_processed_path, option, \"df_best_{}_{}.csv\".format(article_type, option))))\n",
    "df = pd.concat(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Dip-NI94U_YC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2271932"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7bNjYdcJVgov"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ปฏิรูปการศึกษา : มุมมองทางกระบวนทัศน์และบริบทสังคมไทยThe Reformation of Eucation from A Thai Perspectiveกระบวนทัศน์และวิธีคิดแบบแยกส่วน ลดส่วน ได้ทำให้\"การศึกษาเรียนรู้\"ใน หลายทศวรรษที่ผ่านมา กลายเป็นเรื่องของนักวิชาการด้านศึกษาศาสตร์ ครุศาสตร์ หรือเป็นเรื่องของโรงเรียน ครูอาจารย์ กระทรวงศึกษาธิการ ทบวงมหาวิทยาลัยฯ มาอย่างต่อเนื่องยาวนาน (เหมือนกับที่เรื่องสุขภาพเป็นเรื่องของแพทย์และโรงพยาบาล) การจัดการศึกษาภายใต้กระบวนทัศน์และวิธีคิดแบบดังกล่าวของรัฐ ได้ถูกวิพากษ์วิจารณ์และตกเป็นจำเลยจากวิกฤตการณ์ทางสังคมมากมาย อันสะท้อนถึงความล้มเหลวของการจัดการศึกษาเพื่อพัฒนามนุษย์ (ปัญหาศีลธรรมเสื่อมถอย ยาเสพติด การขาดจิตสำนึกทางสังคม ฯลฯ) ซึ่งสังคมร่วมกันสรุปว่า เกิดจากความล้มเหลวของระบบการศึกษาในกระบวนทัศน์แบบแยกส่วน นำมาสู่การปฏิรูปการศึกษาที่กำลังดำเนินการอยู่ในปัจจุบัน ด้วยเป้าหมายเพื่อสร้างการเรียนรู้แบบองค์รวม ที่จะทำให้\"ผู้เรียนเก่ง-ดี-มีความสุข\"คำถามที่ผู้เขียนสนใจในการปฏิรูปการศึกษาที่ดำเนินการในปัจจุบัน คือ๑.การปฏิรูปการศึกษาในปัจจุบันดำเนินการภายใต้กระบวนทัศน์แบบบูรณาการ (องค์รวม)ตามที่'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some text in this corpus\n",
    "all_text = \"\".join(df[\"char\"].tolist())\n",
    "all_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18d3EXoPWg7o"
   },
   "source": [
    "### <font color=blue>Question 4</font>\n",
    "Using PyThaiNLP `newmm`, how many words did you get in the BEST corpus (test)? [Runtime is around 7 mins] What are the accuracy, f1, precision, recall scores for each character?\n",
    "\n",
    "Answer this question in question #4 in MyCourseVille. Also print out the answer in this notebook as well.\n",
    "\n",
    "_Question: What main metric should we look at? Why?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "2RUPPoxLWZr8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 tokens: ['ปฏิรูป', 'การศึกษา', ' ', ':', ' ', 'มุมมอง', 'ทาง', 'กระบวนทัศน์', 'และ', 'บริบท']\n",
      "Total tokens: 569631\n"
     ]
    }
   ],
   "source": [
    "####FILL CODE HERE####\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(all_text, engine=\"newmm\")\n",
    "print(f\"First 10 tokens: {tokens[:10]}\")\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "o40FPwvIXKIQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_character(_tokens):\n",
    "    char_list = [0] * len(\"\".join(_tokens))\n",
    "    char_count = 0\n",
    "    for word in _tokens:\n",
    "        char_list[char_count] = 1\n",
    "        char_count += len(word)\n",
    "    return char_list\n",
    "\n",
    "\n",
    "chars = convert_to_character(tokens)\n",
    "chars[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "5NXWxbiWZHSd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1858.62s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "F1 score: 0.8925828735253718\n",
      "Precision score: 0.9422661336900555\n",
      "Recall score: 0.8478765332638281\n",
      "Accuracy score: 0.9431373826329309\n"
     ]
    }
   ],
   "source": [
    "%pip install -q scikit-learn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "####FILL CODE HERE####\n",
    "\n",
    "my_tokens_flag = convert_to_character(tokens)\n",
    "true_tokens_flag = df[\"target\"].astype(int).tolist()\n",
    "\n",
    "f1 = f1_score(true_tokens_flag, my_tokens_flag)\n",
    "precision = precision_score(true_tokens_flag, my_tokens_flag)\n",
    "recall = recall_score(true_tokens_flag, my_tokens_flag)\n",
    "accuracy = accuracy_score(true_tokens_flag, my_tokens_flag)\n",
    "\n",
    "print(f\"F1 score: {f1}\")\n",
    "print(f\"Precision score: {precision}\")\n",
    "print(f\"Recall score: {recall}\")\n",
    "print(f\"Accuracy score: {accuracy}\")\n",
    "\n",
    "######################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
